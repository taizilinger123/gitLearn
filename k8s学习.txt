yum  install  -y  epel-release  wget net-tools telnet tree nmap  sysstat lrzsz dos2unix bind-utils
[root@hdss-22 ~]#  systemctl  restart   chronyd
[root@hdss-22 ~]#  systemctl  status  chronyd
 chronyc sources -v
[root@hdss-21 bin]# cat  /etc/hosts
192.168.26.11   hdss-11
192.168.26.12   hdss-12
192.168.26.21   hdss-21
192.168.26.22   hdss-22
192.168.26.200  hdss-200
echo  "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf && sysctl -p 
#hdss-11上
yum  install  -y  bind  
[root@hdss-11 ~]# vim  /etc/named.conf 
listen-on port 53 { 192.168.26.11; };
allow-query     { any; };
forwarders      { 192.168.26.1; };
dnssec-enable no;
dnssec-validation no;
[root@hdss-11 ~]# named-checkconf
[root@hdss-11 ~]# vim  /etc/named.rfc1912.zones 
zone "host.com" IN {
       type master;
	   file "host.com.zone";
	   allow-update { 192.168.26.11; };
};

zone "od.com" IN {
       type master;
       file "od.com.zone";
       allow-update { 192.168.26.11; };	   
};
[root@hdss-11 ~]# vim /var/named/host.com.zone 
$ORIGIN host.com.
$TTL 600	; 10 minutes
@	IN SOA	dns.host.com. dnsadmin.host.com. (
					2020061401	; serial
					10800		; refresh (3 hours)
					900			; retry (15 minutes)
					604800		; expire (1 week)
					86400		; minimum (1 day)
					)
			NS	dns.host.com.
$TTL 60	; 1 minute
dns				A	192.168.26.11
HDSS-11		A	192.168.26.11
HDSS-12		A	192.168.26.12
HDSS-21		A	192.168.26.21
HDSS-22		A	192.168.26.22
HDSS-200	    A	192.168.26.200
[root@hdss-11 ~]# vim /var/named/od.com.zone 
$ORIGIN od.com.
$TTL 600	; 10 minutes
@	IN SOA	dns.od.com. dnsadmin.od.com. (
					2020061401	; serial
					10800		; refresh (3 hours)
					900			; retry (15 minutes)
					604800		; expire (1 week)
					86400		; minimum (1 day)
					)
			NS	dns.od.com.
$TTL 60	; 1 minute
dns				A	192.168.26.11
[root@hdss-11 named]# named-checkconf
[root@hdss-11 named]# systemctl  start  named
[root@hdss-11 named]# netstat  -tunalp  |  grep  53
[root@hdss-11 named]# dig  -t A  hdss-21.host.com @192.168.26.11 +short
192.168.26.21
[root@hdss-11 named]# dig  -t A  hdss-200.host.com @192.168.26.11 +short
192.168.26.200
[root@hdss-11 named]# cat   /etc/sysconfig/network-scripts/ifcfg-ens33   
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR=192.168.26.11
NETMASK=255.255.255.0
GATEWAY=192.168.26.1
DNS1=192.168.26.11         #每个机器都改这里
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
UUID=99f1d30e-e8a7-454c-bda2-4523e8d2b135
DEVICE=ens33
ONBOOT=yes
[root@hdss-11 named]# systemctl   restart  network 
[root@hdss-11 named]# cat  /etc/resolv.conf   #每个机器都改这里
# Generated by NetworkManager
nameserver 192.168.26.11
[root@hdss-11 named]# ping  baidu.com
PING baidu.com (39.156.69.79) 56(84) bytes of data.
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=1 ttl=128 time=43.3 ms
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=2 ttl=128 time=33.0 ms
[root@hdss-11 named]# ping  hdss-21.host.com
PING HDSS-21.host.com (192.168.26.21) 56(84) bytes of data.
64 bytes from 192.168.26.21 (192.168.26.21): icmp_seq=1 ttl=64 time=2.52 ms
64 bytes from 192.168.26.21 (192.168.26.21): icmp_seq=2 ttl=64 time=0.368 ms

[root@hdss-11 named]# cat    /etc/resolv.conf 
# Generated by NetworkManager
search host.com    #加这条就可以ping  hdss-200不需要加hdss-200.host.com
nameserver 192.168.26.11
[root@hdss-11 named]# ping  hdss-200
PING HDSS-200.host.com (192.168.26.200) 56(84) bytes of data.
64 bytes from 192.168.26.200 (192.168.26.200): icmp_seq=1 ttl=64 time=1.18 ms
64 bytes from 192.168.26.200 (192.168.26.200): icmp_seq=2 ttl=64 time=0.284 ms
把win7里面的vmnet8的首选dns改为192.168.26.11
hdss-200上
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64  -O /usr/bin/cfssl 
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64  -O /usr/bin/cfssl-json 
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64  -O /usr/bin/cfssl-certinfo 
chmod +x /usr/bin/cfssl* 
[root@hdss-200 ~]# which  cfssl 
/usr/bin/cfssl
[root@hdss-200 ~]# which  cfssl-json
/usr/bin/cfssl-json
[root@hdss-200 ~]# which  cfssl-certinfo 
/usr/bin/cfssl-certinfo
[root@hdss-200 ~]# cd  /opt
[root@hdss-200 opt]# mkdir certs
[root@hdss-200 opt]# cd  certs/
[root@hdss-200 certs]# pwd
/opt/certs
[root@hdss-200 certs]# cat  /opt/certs/ca-csr.json 
{
  "CN": "OldboyEdu",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "od",
      "OU": "ops"
    }
  ],
  "ca": {
      "expiry": "175200h"  
  }
}
[root@hdss-200 certs]# ll
total 4
-rw-r--r-- 1 root root 261 Jun 14 12:24 ca-csr.json
[root@hdss-200 certs]# cfssl gencert -initca  ca-csr.json
2020/06/14 12:28:50 [INFO] generating a new CA key and certificate from CSR
2020/06/14 12:28:50 [INFO] generate received request
2020/06/14 12:28:50 [INFO] received CSR
2020/06/14 12:28:50 [INFO] generating key: rsa-2048
2020/06/14 12:28:51 [INFO] encoded CSR
2020/06/14 12:28:51 [INFO] signed certificate with serial number 368635411612847701070385821800729378027009844386
{"cert":"-----BEGIN CERTIFICATE-----\nMIIDtDCCApygAwIBAgIUQJIu/ajtwc8+sKdXWs4iv1+FpKIwDQYJKoZIhvcNAQEL\nBQAwYDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl\naUppbmcxCzAJBgNVBAoTAm9kMQwwCgYDVQQLEwNvcHMxEjAQBgNVBAMTCU9sZGJv\neUVkdTAeFw0yMDA2MTQwNDI0MDBaFw00MDA2MDkwNDI0MDBaMGAxCzAJBgNVBAYT\nAkNOMRAwDgYDVQQIEwdCZWlKaW5nMRAwDgYDVQQHEwdCZWlKaW5nMQswCQYDVQQK\nEwJvZDEMMAoGA1UECxMDb3BzMRIwEAYDVQQDEwlPbGRib3lFZHUwggEiMA0GCSqG\nSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCxzVlkxMpASz5qEv2n4Kzn6DVjrLVscRDA\ntD0PAxU5YDJAa0FMKXumHCXWVGTABCpKc8PEiSP7uGpq5F0fhSvOLO9qK9KhIUNw\nPbzAvfP93LiJTDQhcz6osgmOyRs3dtWy3qdnYIaIoKNormILYe54zm011fT55aDh\nzrxqUOjE+68tKaT/ny7vuhxhYf8itz7ACloCeIpS74yO6JDaPNyPmr4J8HwtCGWF\ncUuwpiCUGTakzKmy1O5BCyHLqR0pYdukMknl3WBO8b+0rQetBD/AevUsIlmNH/vY\nHRRHUEljpIVlL16Y4V+Rxqe/mC6x/Roal/1hNSFFkbQ7GQLw6QQHAgMBAAGjZjBk\nMA4GA1UdDwEB/wQEAwIBBjASBgNVHRMBAf8ECDAGAQH/AgECMB0GA1UdDgQWBBQl\n7XtlXJv22H8Zl7O1mvVZK7NUVDAfBgNVHSMEGDAWgBQl7XtlXJv22H8Zl7O1mvVZ\nK7NUVDANBgkqhkiG9w0BAQsFAAOCAQEAAeplBg7oC8klxXqn0JNinUzkQBjJI0p7\ngU4C7IH37fI4HoUb+8lWccwC3ItQDASkHzITgkxeOghuHT0WL5mdAnOtJtHXN5Bu\n92fVcAgk6en+uoy9GeBV0ORDxTH6CffgOcdp49oszKPxBHAjNeKf3PSJr7c5v7EX\nTQHYgS8mvy1WpdQbKi8uIzzy+QeinF6josmfhV7bYihNchDb1y4hNwG2ua5S8ZeG\nBjxW5Jmxsc8w5QiTrdutxmpvVepW7OPjdBOU0hwDTr1LwB77tHz+1FUx9TM8cotP\nxzGQ5aUWDgT9RZCOhKPMbiUf9PjZv+h78ESeI0nMD2yCi8rXw7UiHg==\n-----END CERTIFICATE-----\n","csr":"-----BEGIN CERTIFICATE REQUEST-----\nMIICpTCCAY0CAQAwYDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAO\nBgNVBAcTB0JlaUppbmcxCzAJBgNVBAoTAm9kMQwwCgYDVQQLEwNvcHMxEjAQBgNV\nBAMTCU9sZGJveUVkdTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALHN\nWWTEykBLPmoS/afgrOfoNWOstWxxEMC0PQ8DFTlgMkBrQUwpe6YcJdZUZMAEKkpz\nw8SJI/u4amrkXR+FK84s72or0qEhQ3A9vMC98/3cuIlMNCFzPqiyCY7JGzd21bLe\np2dghoigo2iuYgth7njObTXV9PnloOHOvGpQ6MT7ry0ppP+fLu+6HGFh/yK3PsAK\nWgJ4ilLvjI7okNo83I+avgnwfC0IZYVxS7CmIJQZNqTMqbLU7kELIcupHSlh26Qy\nSeXdYE7xv7StB60EP8B69SwiWY0f+9gdFEdQSWOkhWUvXpjhX5HGp7+YLrH9GhqX\n/WE1IUWRtDsZAvDpBAcCAwEAAaAAMA0GCSqGSIb3DQEBCwUAA4IBAQAupSzFnqcv\neErg6xkGkHhhUi9kFsUWdh549zadNwxD81ZDMIZC5U8zQ7OIvs6atmwJ7F7u31jI\nzLVq//WW9IuAmH3nFN7xogNyDHjfNUgE8/fnBPo/X2UUhst9QZ9HfezIkv65KttT\nI7IO76g/2mKXXmtLGdhZ6cyuJ2gOf8QnibvdJqgZJqXp9X1xUsqLP225IlnX+5MC\nlpUdbrHsqqjSwTrKxWwa68zB56RKQN37Q57CTXm6TFZarIfQnBTgGEhFHWXXE9+r\nRGaUd2SnZ7kM9X6tWx+O5JQqcREAjDX9McIUUDFp/3nDNtj1nUvyCU6fw04snklv\nQ9et0dgO04Yk\n-----END CERTIFICATE REQUEST-----\n","key":"-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAsc1ZZMTKQEs+ahL9p+Cs5+g1Y6y1bHEQwLQ9DwMVOWAyQGtB\nTCl7phwl1lRkwAQqSnPDxIkj+7hqauRdH4UrzizvaivSoSFDcD28wL3z/dy4iUw0\nIXM+qLIJjskbN3bVst6nZ2CGiKCjaK5iC2HueM5tNdX0+eWg4c68alDoxPuvLSmk\n/58u77ocYWH/Irc+wApaAniKUu+MjuiQ2jzcj5q+CfB8LQhlhXFLsKYglBk2pMyp\nstTuQQshy6kdKWHbpDJJ5d1gTvG/tK0HrQQ/wHr1LCJZjR/72B0UR1BJY6SFZS9e\nmOFfkcanv5gusf0aGpf9YTUhRZG0OxkC8OkEBwIDAQABAoIBAElsCgkxtm88Yya8\nduC3R89wNXPWowiH3Cong7tKQ55w2e9+GvXPf48Fup46NWyRi8E48tOa6jANZGyF\npurYPFGkfq0yMYV2TfnwlJxnFHV1iwKvZCuYAnz5IDdFVqbdvLtjdG+dDZEn/LLC\nqBwZMbfTTofXtZToPTph+65PgP+uWpavjauiZKlMAPi9uTLv6eqlUoJUqMgCgve7\n1f/WKmiV/8s7+v3swb5rZYOX5qHg/eJL7Iq8jFiuOiZFjAknj0Ys7m+F9tI9Auc0\nHuudp9UEdtjRu7VVM4GGGLoU2oGbVzhT7Qf7yrrewsbq5NT3NTopEsamaDoR7mj/\nFJa3IeECgYEAyWykUlHwLbGwtMel9rL9l1FEm1gSmizbl1KGHLgez9Ig7Hm4wuaB\nSZ255wG8JS32J5hM86kL9zlvXizDalvGM/bP8PeAJf2Pob0TVWrFJknWdcRidY8V\nOdtvOt4HlVyOQSyrXIK/MhK1lfZ+4hfh4R/ltOU6Hchl4ue91Ur0ujcCgYEA4foz\n8yL6zODCuxxQqOZMP3WDxf2QUK9lvoU0eVbLlvt4DjluAldDiyfUt9ThUKihdSkR\nb1A846UtiQY+gotIUEPwLQ3XX7fvLxPFMIQY8ge/QncPEgCWY77bLSdbGw4ivFIQ\na5Qib5LSny2qOEGQ3g+4yrSEhrb+usZqcLU73LECgYEAsY8f26F3hBBboUFNwAdR\nHQ7616Xv4BEjEmYg6zgzkSe+9jbm9IiV9/seYdFZtbyqfYT4cOLYNXtf/TigmyWD\nt1MfQ2Aqi1LWRtGSNX4FhRcaAn63/MHBCFuz1cTcEoKgtDtYZBityspLYIMGnvBG\nolmn+KsJbC+4ASogYeHNkkkCgYAqPTRcrBspFJIRm3dQOsvhyLt61KyXCFICTK/E\nRn8Dl7mXHxD7CwirCCTwv6/l/b9AQQ7rOZpICuX6CNrCV/qKVkicg0vQrnbc29st\n+GMZSm1rpQmCGe4CcvhQ9lxHMFs5zDUZhrTxGgaYjlaU8Vx8xX+LFDb5X1c53kCj\nFJ094QKBgAGRFdkzMs4Di/enR3M+TnuqWzj4j/uGpsQicQx+XyBh1ilZdP7xOmF3\nbCRLqMKJ2+ud2osYnRATdmTWLJ6vtXThJKhd5uJw3UHuyKGvO3pbZGcIOJzByuMs\nkI/FiEFp9UnrVEikG0oS1Xwcd/P0sUN1W+XqJVc1+MQ7+EX+RxMC\n-----END RSA PRIVATE KEY-----\n"}
[root@hdss-200 certs]# ls
ca-csr.json
[root@hdss-200 certs]# cfssl gencert -initca  ca-csr.json | cfssl-json -bare ca
2020/06/14 12:30:39 [INFO] generating a new CA key and certificate from CSR
2020/06/14 12:30:39 [INFO] generate received request
2020/06/14 12:30:39 [INFO] received CSR
2020/06/14 12:30:39 [INFO] generating key: rsa-2048
2020/06/14 12:30:40 [INFO] encoded CSR
2020/06/14 12:30:40 [INFO] signed certificate with serial number 721569988218003332682219172812919181468096065230
[root@hdss-200 certs]# ll
total 16
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr     #生成
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem #生成
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem     #生成
#在hdss-200,hdss-21,hdss-22上部署docker
yum  remove  -y   docker-1.13.1-161.git64e9980.el7_8.x86_64
yum  remove  -y   docker-common-2:1.13.1-161.git64e9980.el7_8.x86_64
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 
mkdir /etc/docker 
[root@hdss-200 certs]# cat  /etc/docker/daemon.json
{
   "registry-mirrors": ["https://9cpn8tt6.mirror.aliyuncs.com"],
   "graph": "/data/docker",
   "storage-driver": "overlay2",
   "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.od.com"],
   "bip": "172.7.200.1/24",
   "exec-opts": ["native.cgroupdriver=systemd"],
   "live-restore": true
}
[root@hdss-200 ~]# systemctl daemon-reload
[root@hdss-200 ~]# systemctl restart docker
[root@hdss-22 ~]# cat   /etc/docker/daemon.json
{
   "graph": "/data/docker",
   "storage-driver": "overlay2",
   "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.od.com"],
   "bip": "172.7.22.1/24",
   "exec-opts": ["native.cgroupdriver=systemd"],
   "live-restore": true
}
[root@hdss-21 ~]# cat   /etc/docker/daemon.json
{
   "graph": "/data/docker",
   "storage-driver": "overlay2",
   "insecure-registries": ["registry.access.redhat.com","quay.io","harbor.od.com"],
   "bip": "172.7.21.1/24",
   "exec-opts": ["native.cgroupdriver=systemd"],
   "live-restore": true
}
[root@hdss-200 certs]# mkdir  -p  /data/docker  /etc/docker 
[root@hdss-200 certs]# systemctl  start  docker
[root@hdss-200 certs]# docker  info/version/ps -a 
https://github.com/goharbor/harbor 
在hdss-200上 
[root@hdss-200 certs]# cd   /opt/
[root@hdss-200 opt]# mkdir  src
[root@hdss-200 src]# pwd
/opt/src
[root@hdss-200 src]# wget  https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.3.tgz
[root@hdss-200 src]# tar xf harbor-offline-installer-v1.8.3.tgz  -C  /opt 
[root@hdss-200 src]# cd  /opt 
[root@hdss-200 src]# mv  harbor/ harbor-v1.8.3
[root@hdss-200 src]# ln -s /opt/harbor-v1.8.3/  /opt/harbor 
[root@hdss-200 harbor]# grep  hostname  /opt/harbor/harbor.yml
#The IP address or hostname to access admin UI and registry service.
hostname = harbor.od.com
http:
   port: 180 
data_volume: /data/harbor 
location: /data/harbor/logs 
mkdir -p  /data/harbor/logs
[root@hdss-200 harbor]# yum  install  -y  docker-compose  -y 
[root@hdss-200 harbor]# rpm  -qa  docker-compose
docker-compose-1.18.0-4.el7.noarch
[root@hdss-200 harbor]#sh  /opt/harbor/install.sh    #当http://harbor.od.com网页登录不了时候这个重新跑一下
[root@hdss-200 harbor]# pwd
/opt/harbor
[root@hdss-200 harbor]# docker-compose start
Starting log         ... done
Starting registry    ... done
Starting mysql       ... done
Starting adminserver ... done
Starting ui          ... done
Starting redis       ... done
Starting jobservice  ... done
Starting proxy       ... done
[root@hdss-200 harbor]# docker-compose  ps 
      Name                     Command               State             Ports          
--------------------------------------------------------------------------------------
harbor-core         /harbor/start.sh                 Up                               
harbor-db           /entrypoint.sh postgres          Up      5432/tcp                 
harbor-jobservice   /harbor/start.sh                 Up                               
harbor-log          /bin/sh -c /usr/local/bin/ ...   Up      127.0.0.1:1514->10514/tcp
harbor-portal       nginx -g daemon off;             Up      80/tcp                   
nginx               nginx -g daemon off;             Up      0.0.0.0:180->80/tcp      
redis               docker-entrypoint.sh redis ...   Up      6379/tcp                 
registry            /entrypoint.sh /etc/regist ...   Up      5000/tcp                 
registryctl         /harbor/start.sh                 Up                               
[root@hdss-200 harbor]# docker  ps  -a
CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS                   PORTS                       NAMES
73d2202c0c8b        goharbor/nginx-photon:v1.8.3                        "nginx -g 'daemon of…"   2 minutes ago       Up 2 minutes (healthy)   0.0.0.0:180->80/tcp         nginx
260d7c590cfb        goharbor/harbor-portal:v1.8.3                       "nginx -g 'daemon of…"   2 minutes ago       Up 2 minutes (healthy)   80/tcp                      harbor-portal
4716a3a86f7b        goharbor/harbor-jobservice:v1.8.3                   "/harbor/start.sh"       2 minutes ago       Up 2 minutes                                         harbor-jobservice
b6d69794935d        goharbor/harbor-core:v1.8.3                         "/harbor/start.sh"       2 minutes ago       Up 2 minutes (healthy)                               harbor-core
f7923d836c26        goharbor/harbor-registryctl:v1.8.3                  "/harbor/start.sh"       2 minutes ago       Up 2 minutes (healthy)                               registryctl
a91e52845bb9        goharbor/registry-photon:v2.7.1-patch-2819-v1.8.3   "/entrypoint.sh /etc…"   2 minutes ago       Up 2 minutes (healthy)   5000/tcp                    registry
fa25086ef3db        goharbor/harbor-db:v1.8.3                           "/entrypoint.sh post…"   2 minutes ago       Up 2 minutes (healthy)   5432/tcp                    harbor-db
b198513b3e81        goharbor/redis-photon:v1.8.3                        "docker-entrypoint.s…"   2 minutes ago       Up 2 minutes             6379/tcp                    redis
5de9d9201236        goharbor/harbor-log:v1.8.3                          "/bin/sh -c /usr/loc…"   2 minutes ago       Up 2 minutes (healthy)   127.0.0.1:1514->10514/tcp   harbor-log
 
[root@hdss-200 harbor]#sh  /opt/harbor/install.sh    #当http://harbor.od.com网页登录不了时候这个重新跑一下  
###################################################
[root@hdss-200 harbor]# vim  harbor.yml 
[root@hdss-200 harbor]# docker-compose down
[root@hdss-200 harbor]# ./install.sh 
###################################################
[root@hdss-200 harbor]# curl  harbor.od.com
<!doctype html>
<html>

<head>
    <meta charset="utf-8">
    <title>Harbor</title>
    <base href="/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/x-icon" href="favicon.ico?v=2">
<link rel="stylesheet" href="styles.c1fdd265f24063370a49.css"></head>

<body>
    <harbor-app>
        <div class="spinner spinner-lg app-loading">
            Loading...
        </div>
    </harbor-app>
<script type="text/javascript" src="runtime.26209474bfa8dc87a77c.js"></script><script type="text/javascript" src="scripts.c04548c4e6d1db502313.js"></script><script type="text/javascript" src="main.144e8ccd474a28572e81.js"></script></body>
                                                    
[root@hdss-200 harbor]# docker  ps  -a 
 docker  rm  -f  fb7e89f8277c
[root@hdss-200 harbor]# yum  install  -y  nginx
/etc/nginx/conf.d/harbor.od.com.conf 
server {
      listen   80;
	  server_name  harbor.od.com;
	  client_max_body_size  1000m;
      location / {
	     proxy_pass http://127.0.0.1:180;
	  }
}
[root@hdss-200 harbor]# nginx -t 
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
[root@hdss-200 harbor]# systemctl  restart  nginx 
[root@hdss-200 harbor]# systemctl  enable   nginx 
在hdss-11上：
[root@hdss-11 named]# cat   /var/named/od.com.zone 
$ORIGIN od.com.
$TTL 600	; 10 minutes
@	IN SOA	dns.od.com. dnsadmin.od.com. (
					2020061402	; serial     #2020061402改
					10800		; refresh (3 hours)
					900			; retry (15 minutes)
					604800		; expire (1 week)
					86400		; minimum (1 day)
					)
			NS	dns.od.com.
$TTL 60	; 1 minute
dns				A	192.168.26.11
harbor				A	192.168.26.200       #添加
[root@hdss-11 named]# systemctl  restart  named
[root@hdss-11 named]# dig  -t  A  harbor.od.com +short 
192.168.26.200
[root@hdss-200 harbor]# curl  harbor.od.com
浏览器打开http://harbor.od.com
admin 
Harbor12345
创建一个public项目
[root@hdss-200 harbor]# docker pull  nginx:1.7.9
<===>docker pull docker.io/library/nginx:v1.7.9
[root@hdss-200 harbor]# docker images|grep  1.7.9
nginx                         1.7.9               84581e99d807        5 years ago         91.7MB
[root@hdss-200 harbor]# docker tag 84581e99d807  harbor.od.com/public/nginx:v1.7.9
[root@hdss-200 harbor]# docker login harbor.od.com
Username: admin
Password: Harbor12345
[root@hdss-200 harbor]# docker push   harbor.od.com/public/nginx:v1.7.9

在hdss-200上：
[root@hdss-200 certs]# cat  ca-config.json 
{
    "signing": {
        "default": {
            "expiry": "175200h"
        },
        "profiles": {
            "server": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "175200h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}

[root@hdss-200 certs]# vim  etcd-peer-csr.json 
{
    "CN": "k8s-etcd",
    "hosts": [
        "192.168.26.11",
        "192.168.26.12",
        "192.168.26.21",
        "192.168.26.22"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
[root@hdss-200 certs]# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  etcd-peer-csr.json
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json |cfssl-json -bare etcd-peer
[root@hdss-200 certs]# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  etcd-peer.csr  etcd-peer-csr.json  etcd-peer-key.pem  etcd-peer.pem
在hdss-12/21/22上：
[root@hdss-12 ~]# cd  /opt
[root@hdss-12 opt]# mkdir src
[root@hdss-12 opt]# cd   src/
[root@hdss-12 src]# ls
[root@hdss-12 src]# pwd
/opt/src
[root@hdss-12 src]# useradd  -s  /sbin/nologin -M etcd
useradd: user 'etcd' already exists
[root@hdss-12 src]# id  etcd 
uid=996(etcd) gid=993(etcd) groups=993(etcd)
https://github.com/etcd-io/etcd/releases/tag/v3.1.20
[root@hdss-12 src]# tar  xfv  etcd-v3.1.20-linux-amd64.tar.gz   -C  /opt 
[root@hdss-12 src]# cd  /opt/
[root@hdss-12 opt]# ls
etcd-v3.1.20-linux-amd64  src
[root@hdss-12 opt]# mv  etcd-v3.1.20-linux-amd64/   etcd-v3.1.20
[root@hdss-12 opt]# ln -s   /opt/etcd-v3.1.20/   /opt/etcd
[root@hdss-12 etcd]#  mkdir -p /opt/etcd/certs /data/etcd /data/logs/etcd-server 
[root@hdss-12 etcd]# cd   certs 
[root@hdss-12 certs]# ls
[root@hdss-12 certs]# pwd
/opt/etcd/certs
[root@hdss-12 certs]# scp  hdss-200:/opt/certs/ca.pem  .
The authenticity of host 'hdss-200 (192.168.26.200)' can't be established.
ECDSA key fingerprint is SHA256:92jl5ZCWYIY9tMQM2/H3m6dsnmAuqCsvpgL5FU2lpeM.
ECDSA key fingerprint is MD5:86:df:cd:53:64:33:e2:c7:62:b9:0a:6c:39:b3:ea:a6.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'hdss-200,192.168.26.200' (ECDSA) to the list of known hosts.
root@hdss-200's password: 
ca.pem                                                                                                                                                       100% 1346   437.3KB/s   00:00    
[root@hdss-12 certs]# ls
ca.pem
[root@hdss-12 certs]# scp  hdss-200:/opt/certs/etcd-peer.pem  .
root@hdss-200's password: 
etcd-peer.pem                                                                                                                                                100% 1428   246.6KB/s   00:00    
[root@hdss-12 certs]# scp  hdss-200:/opt/certs/etcd-peer-key.pem  .
root@hdss-200's password: 
etcd-peer-key.pem                                                                                                                                            100% 1679   518.8KB/s   00:00    
[root@hdss-12 certs]# ls
ca.pem  etcd-peer-key.pem  etcd-peer.pem
[root@hdss-12 etcd]# cat   /opt/etcd/etcd-server-startup.sh
#!/bin/sh
/opt/etcd/etcd --name etcd-server-7-12 \
       --data-dir /data/etcd/etcd-server \
       --listen-peer-urls https://192.168.26.12:2380 \
       --listen-client-urls https://192.168.26.12:2379,http://127.0.0.1:2379 \
       --quota-backend-bytes 8000000000 \
       --initial-advertise-peer-urls https://192.168.26.12:2380 \
       --advertise-client-urls https://192.168.26.12:2379,http://127.0.0.1:2379 \
       --initial-cluster  etcd-server-7-12=https://192.168.26.12:2380,etcd-server-7-21=https://192.168.26.21:2380,etcd-server-7-22=https://192.168.26.22:2380 \
       --ca-file /opt/etcd/certs/ca.pem \
       --cert-file /opt/etcd/certs/etcd-peer.pem \
       --key-file /opt/etcd/certs/etcd-peer-key.pem \
       --client-cert-auth  \
       --trusted-ca-file /opt/etcd/certs/ca.pem \
       --peer-ca-file /opt/etcd/certs/ca.pem \
       --peer-cert-file /opt/etcd/certs/etcd-peer.pem \
       --peer-key-file /opt/etcd/certs/etcd-peer-key.pem \
       --peer-client-cert-auth \
       --peer-trusted-ca-file /opt/etcd/certs/ca.pem \
       --log-output stdout
[root@hdss-12 etcd]# chmod  +x   etcd-server-startup.sh 
[root@hdss-12 etcd]# chown  -R  etcd.etcd  /opt/etcd-v3.1.20/
[root@hdss-12 etcd]# chown  -R  etcd.etcd  /data/etcd/
[root@hdss-12 etcd]# chown  -R  etcd.etcd  /data/logs/etcd-server/
[root@hdss-12 etcd]# yum  install  -y  supervisor
[root@hdss-12 etcd]# systemctl  start  supervisord
[root@hdss-12 etcd]# systemctl  enable   supervisord
[root@hdss-12 etcd]# cat  /etc/supervisord.d/etcd-server.ini
[program:etcd-server-7-12]
command=/opt/etcd/etcd-server-startup.sh                        ; the program (relative uses PATH, can take args)
numprocs=1                                                      ; number of processes copies to start (def 1)
directory=/opt/etcd                                             ; directory to cwd to before exec (def no cwd)
autostart=true                                                  ; start at supervisord start (default: true)
autorestart=true                                                ; retstart at unexpected quit (default: true)
startsecs=30                                                    ; number of secs prog must stay running (def. 1)
startretries=3                                                  ; max # of serial start failures (default 3)
exitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                 ; signal used to kill process (default TERM)
stopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)
user=etcd                                                       ; setuid to this UNIX account to run the program
redirect_stderr=true                                            ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/etcd-server/etcd.stdout.log           ; stdout log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                     ; emit events on stdout writes (default false)
killasgroup=true
stopasgroup=true
[root@hdss-12 etcd]# supervisorctl update
etcd-server-7-12: added process group
[root@hdss-12 etcd]# supervisorctl status
etcd-server-7-12                 RUNNING   pid 50389, uptime 0:00:33
[root@hdss-21 etcd]# supervisorctl stop  all
etcd-server-7-21: stopped
[root@hdss-21 etcd]# supervisorctl start   all
etcd-server-7-21: started
[root@hdss-12 etcd]# tail  -fn  200  /data/logs/etcd-server/etcd.stdout.log 
[root@hdss-12 etcd]# netstat  -tunlp  |  grep  etcd
tcp        0      0 192.168.26.12:2379      0.0.0.0:*               LISTEN      50390/etcd          
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      50390/etcd          
tcp        0      0 192.168.26.12:2380      0.0.0.0:*               LISTEN      50390/etcd    
[root@hdss-12 etcd]#  ./etcdctl member list
46b9a327aae1e4d2: name=etcd-server-7-22 peerURLs=https://192.168.26.22:2380 clientURLs=http://127.0.0.1:2379,https://192.168.26.22:2379 isLeader=true
4858c97d15f32459: name=etcd-server-7-21 peerURLs=https://192.168.26.21:2380 clientURLs=http://127.0.0.1:2379,https://192.168.26.21:2379 isLeader=false
b0123fbb75c41193: name=etcd-server-7-12 peerURLs=https://192.168.26.12:2380 clientURLs=http://127.0.0.1:2379,https://192.168.26.12:2379 isLeader=false
[root@hdss-12 etcd]# ./etcdctl cluster-health
member 46b9a327aae1e4d2 is healthy: got healthy result from http://127.0.0.1:2379
member 4858c97d15f32459 is healthy: got healthy result from http://127.0.0.1:2379
member b0123fbb75c41193 is healthy: got healthy result from http://127.0.0.1:2379
cluster is healthy
在hdss-21/22上：
[root@hdss-21 ~]# mv  kubernetes-server-linux-amd64.tar.gz   /opt/src/
[root@hdss-21 src]# tar  xfv   kubernetes-server-linux-amd64.tar.gz   -C  /opt
[root@hdss-21 src]# cd   /opt
[root@hdss-21 opt]# ls
containerd  etcd  etcd-v3.1.20  kubernetes  src
[root@hdss-21 opt]# mv  kubernetes/   kubernetes-v1.15.2
[root@hdss-21 opt]# ls
containerd  etcd  etcd-v3.1.20  kubernetes-v1.15.2  src
[root@hdss-21 opt]# ln  -s   /opt/kubernetes-v1.15.2/   /opt/kubernetes
[root@hdss-21 opt]# cd   kubernetes
[root@hdss-21 kubernetes]# ls
addons  kubernetes-src.tar.gz  LICENSES  server
[root@hdss-21 kubernetes]# 
[root@hdss-21 kubernetes]# ll
total 27884
drwxr-xr-x 2 root root        6 May 16  2018 addons
-rw-r--r-- 1 root root 23298044 May 16  2018 kubernetes-src.tar.gz
-rw-r--r-- 1 root root  5254932 May 16  2018 LICENSES
drwxr-xr-x 3 root root       17 May 16  2018 server
[root@hdss-21 kubernetes]# rm  -rf  kubernetes-src.tar.gz 
[root@hdss-21 bin]# pwd
/opt/kubernetes/server/bin
[root@hdss-21 bin]# rm  -rf  *.tar
[root@hdss-21 bin]# rm  -f  *_tag
[root@hdss-21 bin]# ll
total 1234032
-rwxr-xr-x 1 root root  55195780 May 16  2018 apiextensions-apiserver
-rwxr-xr-x 1 root root 109421915 May 16  2018 cloud-controller-manager
-rwxr-xr-x 1 root root 237016512 May 16  2018 hyperkube
-rwxr-xr-x 1 root root 136820957 May 16  2018 kubeadm
-rwxr-xr-x 1 root root  54034392 May 16  2018 kube-aggregator
-rwxr-xr-x 1 root root 193657633 May 16  2018 kube-apiserver
-rwxr-xr-x 1 root root 128546640 May 16  2018 kube-controller-manager
-rwxr-xr-x 1 root root  52500769 May 16  2018 kubectl
-rwxr-xr-x 1 root root  56096107 May 16  2018 kubefed
-rwxr-xr-x 1 root root 138185536 May 16  2018 kubelet
-rwxr-xr-x 1 root root  48157031 May 16  2018 kube-proxy
-rwxr-xr-x 1 root root  53989846 May 16  2018 kube-scheduler
[root@hdss-200 certs]# ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  client-csr.json  etcd-peer.csr  etcd-peer-csr.json  etcd-peer-key.pem  etcd-peer.pem
[root@hdss-200 certs]# cat  client-csr.json 
{
        "CN": "k8s-node",
        "hosts": [],
        "key": {
                "algo": "rsa",
                "size": 2048
        },
        "names": [
                {
                        "C": "CN",
                        "ST": "beijing",
                        "L": "beijing",
                        "O": "od",
                        "OU": "ops"
                }
        ]
}
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=client client-csr.json | cfssl-json -bare client
[root@hdss-200 certs]# ll
total 52
-rw-r--r-- 1 root root  836 Jun 14 21:48 ca-config.json
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem
-rw-r--r-- 1 root root  993 Jun 15 23:29 client.csr
-rw-r--r-- 1 root root  392 Jun 15 23:24 client-csr.json
-rw------- 1 root root 1675 Jun 15 23:29 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:29 client.pem
-rw-r--r-- 1 root root 1062 Jun 14 21:58 etcd-peer.csr
-rw-r--r-- 1 root root  380 Jun 14 21:54 etcd-peer-csr.json
-rw------- 1 root root 1679 Jun 14 21:58 etcd-peer-key.pem
-rw-r--r-- 1 root root 1428 Jun 14 21:58 etcd-peer.pem
[root@hdss-200 certs]# cat  apiserver-csr.json
{
    "CN": "k8s-apiserver",
    "hosts": [
      "127.0.0.1",
      "192.168.0.1",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local",
      "192.168.26.10",
      "192.168.26.21",
      "192.168.26.22",
      "192.168.26.23"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=server apiserver-csr.json | cfssl-json -bare apiserver
[root@hdss-200 certs]# ll
total 68
-rw-r--r-- 1 root root 1249 Jun 15 23:41 apiserver.csr
-rw-r--r-- 1 root root  562 Jun 15 23:41 apiserver-csr.json
-rw------- 1 root root 1675 Jun 15 23:41 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:41 apiserver.pem
-rw-r--r-- 1 root root  836 Jun 14 21:48 ca-config.json
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem
-rw-r--r-- 1 root root  993 Jun 15 23:29 client.csr
-rw-r--r-- 1 root root  392 Jun 15 23:24 client-csr.json
-rw------- 1 root root 1675 Jun 15 23:29 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:29 client.pem
-rw-r--r-- 1 root root 1062 Jun 14 21:58 etcd-peer.csr
-rw-r--r-- 1 root root  380 Jun 14 21:54 etcd-peer-csr.json
-rw------- 1 root root 1679 Jun 14 21:58 etcd-peer-key.pem
-rw-r--r-- 1 root root 1428 Jun 14 21:58 etcd-peer.pem
[root@hdss-21 bin]# mkdir  cert 
[root@hdss-21 bin]# ls
apiextensions-apiserver  cert  cloud-controller-manager  hyperkube  kubeadm  kube-aggregator  kube-apiserver  kube-controller-manager  kubectl  kubefed  kubelet  kube-proxy  kube-scheduler
[root@hdss-21 cert]# cat  /etc/resolv.conf 
# Generated by NetworkManager
search host.com
nameserver 192.168.26.11
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/ca.pem   .
root@hdss-200's password: 
ca.pem                                                                                                                                                       100% 1346   615.9KB/s   00:00    
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/ca-key.pem   .
root@hdss-200's password: 
ca-key.pem                                                                                                                                                   100% 1679    18.4KB/s   00:00    
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/client.pem   .
root@hdss-200's password: 
client.pem                                                                                                                                                   100% 1363   571.7KB/s   00:00    
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/client-key.pem   .
root@hdss-200's password: 
client-key.pem                                                                                                                                               100% 1675     9.4KB/s   00:00    
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/apiserver.pem   .
root@hdss-200's password: 
apiserver.pem                                                                                                                                                100% 1598   266.1KB/s   00:00    
[root@hdss-21 cert]# scp  hdss-200:/opt/certs/apiserver-key.pem   .
root@hdss-200's password: 
apiserver-key.pem 
[root@hdss-21 cert]# ll
total 24
-rw------- 1 root root 1675 Jun 15 23:53 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:53 apiserver.pem
-rw------- 1 root root 1679 Jun 15 23:52 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 15 23:52 ca.pem
-rw------- 1 root root 1675 Jun 15 23:53 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:53 client.pem
[root@hdss-21 bin]# mkdir  conf 
[root@hdss-21 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss-21 conf]# cat  audit.yaml 
apiVersion: audit.k8s.io/v1beta1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]
 
  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]
 
  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]
 
  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"
 
  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]
 
  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]
 
  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.
 
  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
[root@hdss-21 bin]# cat  kube-apiserver.sh 
#!/bin/bash
./kube-apiserver \
  --apiserver-count 2 \
  --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \
  --audit-policy-file ./conf/audit.yaml \
  --authorization-mode RBAC \
  --client-ca-file ./cert/ca.pem \
  --requestheader-client-ca-file ./cert/ca.pem \
  --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \
  --etcd-cafile ./cert/ca.pem \
  --etcd-certfile ./cert/client.pem \
  --etcd-keyfile ./cert/client-key.pem \
  --etcd-servers https://192.168.26.12:2379,https://192.168.26.21:2379,https://192.168.26.22:2379 \
  --service-account-key-file ./cert/ca-key.pem \
  --service-cluster-ip-range 192.168.0.0/16 \
  --service-node-port-range 3000-29999 \
  --target-ram-mb=1024 \
  --kubelet-client-certificate ./cert/client.pem \
  --kubelet-client-key ./cert/client-key.pem \
  --log-dir  /data/logs/kubernetes/kube-apiserver \
  --tls-cert-file ./cert/apiserver.pem \
  --tls-private-key-file ./cert/apiserver-key.pem \
  --v 2
[root@hdss-21 bin]# chmod  +x  kube-apiserver.sh 
[root@hdss-21 bin]# cat   /etc/supervisord.d/kube-apiserver.ini
[program:kube-apiserver-7-21]
command=/opt/kubernetes/server/bin/kube-apiserver.sh            ; the program (relative uses PATH, can take args)
numprocs=1                                                      ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                            ; directory to cwd to before exec (def no cwd)
autostart=true                                                  ; start at supervisord start (default: true)
autorestart=true                                                ; retstart at unexpected quit (default: true)
startsecs=30                                                    ; number of secs prog must stay running (def. 1)
startretries=3                                                  ; max # of serial start failures (default 3)
exitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                 ; signal used to kill process (default TERM)
stopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                       ; setuid to this UNIX account to run the program
redirect_stderr=true                                            ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-apiserver/apiserver.stdout.log        ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                     ; emit events on stdout writes (default false)
[root@hdss-21 bin]#  mkdir -p /data/logs/kubernetes/kube-apiserver
[root@hdss-21 bin]# supervisorctl update
kube-apiserver-7-21: added process group
[root@hdss-21 bin]# supervisorctl status 
etcd-server-7-21                 RUNNING   pid 10233, uptime 0:31:00
kube-apiserver-7-21              FATAL     Exited too quickly (process log may have details)
[root@hdss-21 bin]# netstat  -luntp |  grep  kube-api
在hdss-11/12上:
[root@hdss-11 ~]# yum  install  -y  nginx  
[root@hdss-11 ~]# vi /etc/nginx/nginx.conf
在最后添加4层反向代理
stream {
    upstream kube-apiserver {
        server 192.168.26.21:6443     max_fails=3 fail_timeout=30s;
        server 192.168.26.22:6443     max_fails=3 fail_timeout=30s;
    }
    server {
        listen 7443;
        proxy_connect_timeout 2s;
        proxy_timeout 900s;
        proxy_pass kube-apiserver;
    }
}
[root@hdss-11 ~]# nginx  -t 
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
[root@hdss-11 ~]# systemctl  start  nginx 
[root@hdss-11 ~]# systemctl enable   nginx 
Created symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service.
[root@hdss-11 ~]# yum install keepalived -y
[root@hdss-11 ~]# cat   /etc/keepalived/check_port.sh
#!/bin/bash
#keepalived 监控端口脚本
#使用方法：
#在keepalived的配置文件中
#vrrp_script check_port {#创建一个vrrp_script脚本,检查配置
#    script "/etc/keepalived/check_port.sh 6379" #配置监听的端口
#    interval 2 #检查脚本的频率,单位（秒）
#}
CHK_PORT=$1
if [ -n "$CHK_PORT" ];then
        PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`
        if [ $PORT_PROCESS -eq 0 ];then
                echo "Port $CHK_PORT Is Not Used,End."
                exit 1
        fi
else
        echo "Check Port Cant Be Empty!"
fi
[root@hdss-11 ~]#  chmod +x /etc/keepalived/check_port.sh
keepalived主：
[root@hdss-11 ~]# cat  /etc/keepalived/keepalived.conf
! Configuration File for keepalived
 
global_defs {
   router_id 192.168.26.11
 
}
 
vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 7443"
    interval 2
    weight -20
}
 
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 251
    priority 100
    advert_int 1
    mcast_src_ip 192.168.26.11
    nopreempt   #非抢占式 ，当主节点挂了以后，从节点vip飘到从上，主节点恢复以后，不主动飘回主，需要手动重启keepalived
 
    authentication {
        auth_type PASS
        auth_pass 11111111
    }
    track_script {
         chk_nginx
    }
    virtual_ipaddress {
        192.168.26.10
    }
}
keepalived从:
[root@hdss-12 ~]# cat   /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id 192.168.26.12
}
vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 7443"
    interval 2
    weight -20
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    virtual_router_id 251
    mcast_src_ip 192.168.26.12
    priority 90
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 11111111
    }
    track_script {
        chk_nginx
    }
    virtual_ipaddress {
        192.168.26.10
    }
}
[root@hdss-11 ~]# # systemctl start keepalived
[root@hdss-11 ~]# # systemctl enable keepalived
[root@hdss-12 ~]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:29:3c:bc brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.12/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::bc6d:15fc:91cf:b89/64 scope link 
       valid_lft forever preferred_lft forever
    inet6 fe80::d7e8:2f1b:fa63:cf7b/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:33:aa:0d:2c brd ff:ff:ff:ff:ff:ff
    inet 10.255.79.1/24 scope global docker0
       valid_lft forever preferred_lft forever
[root@hdss-12 ~]# netstat -luntp |  grep  7443
tcp        0      0 0.0.0.0:7443            0.0.0.0:*               LISTEN      25289/nginx: master 
[root@hdss-11 keepalived]# service    keepalived  start  
[root@hdss-11 keepalived]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:76:f3:cf brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.11/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet 192.168.26.10/32 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:8a:23:fd:ca brd ff:ff:ff:ff:ff:ff
    inet 10.255.45.1/24 scope global docker0
       valid_lft forever preferred_lft forever
[root@hdss-11 keepalived]# nginx  -s  stop  
[root@hdss-11 keepalived]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:76:f3:cf brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.11/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:8a:23:fd:ca brd ff:ff:ff:ff:ff:ff
    inet 10.255.45.1/24 scope global docker0
       valid_lft forever preferred_lft forever
[root@hdss-12 ~]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:29:3c:bc brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.12/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet 192.168.26.10/32 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::bc6d:15fc:91cf:b89/64 scope link 
       valid_lft forever preferred_lft forever
    inet6 fe80::d7e8:2f1b:fa63:cf7b/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link tentative dadfailed 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:33:aa:0d:2c brd ff:ff:ff:ff:ff:ff
    inet 10.255.79.1/24 scope global docker0
       valid_lft forever preferred_lft forever
[root@hdss-11 keepalived]# nginx  
[root@hdss-11 keepalived]# netstat -luntp |  grep  7443
tcp        0      0 0.0.0.0:7443            0.0.0.0:*               LISTEN      27051/nginx: master 
[root@hdss-11 keepalived]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:76:f3:cf brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.11/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:8a:23:fd:ca brd ff:ff:ff:ff:ff:ff
    inet 10.255.45.1/24 scope global docker0
       valid_lft forever preferred_lft forever
[root@hdss-11 keepalived]# netstat -luntp |  grep  7443
tcp        0      0 0.0.0.0:7443            0.0.0.0:*               LISTEN      27051/nginx: master 
[root@hdss-11 keepalived]# service  keepalived  restart  
[root@hdss-12 ~]#systemctl restart keepalived
[root@hdss-11 keepalived]# ip  addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:76:f3:cf brd ff:ff:ff:ff:ff:ff
    inet 192.168.26.11/24 brd 192.168.26.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet 192.168.26.10/32 scope global ens33        #重启keepalived以后回来了
       valid_lft forever preferred_lft forever
    inet6 fe80::eed0:9c67:5c82:c6a/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:8a:23:fd:ca brd ff:ff:ff:ff:ff:ff
    inet 10.255.45.1/24 scope global docker0
       valid_lft forever preferred_lft forever
在hdss-21/22上：
[root@hdss-21 bin]# cat  /opt/kubernetes/server/bin/kube-controller-manager.sh
#!/bin/sh
./kube-controller-manager \
  --cluster-cidr 172.7.0.0/16 \
  --leader-elect true \
  --log-dir /data/logs/kubernetes/kube-controller-manager \
  --master http://127.0.0.1:8080 \
  --service-account-private-key-file ./cert/ca-key.pem \
  --service-cluster-ip-range 192.168.0.0/16 \
  --root-ca-file ./cert/ca.pem \
  --v 2
[root@hdss-21 bin]# mkdir  -p  /data/logs/kubernetes/kube-controller-manager
[root@hdss-21 bin]# chmod  +x  /opt/kubernetes/server/bin/kube-controller-manager.sh
[root@hdss-21 bin]# cat   /etc/supervisord.d/kube-conntroller-manager.ini
[program:kube-controller-manager-7-21]
command=/opt/kubernetes/server/bin/kube-controller-manager.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                              ; directory to cwd to before exec (def no cwd)
autostart=true                                                                    ; start at supervisord start (default: true)
autorestart=true                                                                  ; retstart at unexpected quit (default: true)
startsecs=30                                                                      ; number of secs prog must stay running (def. 1)
startretries=3                                                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                                                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log  ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                                       ; emit events on stdout writes (default false)
[root@hdss-22 bin]# cat     /etc/supervisord.d/kube-conntroller-manager.ini
[program:kube-controller-manager-7-22]
command=/opt/kubernetes/server/bin/kube-controller-manager.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                              ; directory to cwd to before exec (def no cwd)
autostart=true                                                                    ; start at supervisord start (default: true)
autorestart=true                                                                  ; retstart at unexpected quit (default: true)
startsecs=30                                                                      ; number of secs prog must stay running (def. 1)
startretries=3                                                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                                                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log  ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                                       ; emit events on stdout writes (default false)
[root@hdss-21 bin]# supervisorctl update
kube-controller-manager-7-21: added process group
[root@hdss-21 bin]# supervisorctl status 
etcd-server-7-21                 RUNNING   pid 10233, uptime 1:53:56
kube-apiserver-7-21              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-21     STARTING  
[root@hdss-22 bin]# supervisorctl update
kube-controller-manager-7-22: added process group
[root@hdss-22 bin]# supervisorctl status 
etcd-server-7-22                 RUNNING   pid 803, uptime 23:50:53
kube-apiserver-7-22              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-22     STARTING  
[root@hdss-21 bin]# cat   /opt/kubernetes/server/bin/kube-scheduler.sh
#!/bin/sh
./kube-scheduler \
  --leader-elect  \
  --log-dir /data/logs/kubernetes/kube-scheduler \
  --master http://127.0.0.1:8080 \
  --v 2
[root@hdss-21 bin]# mkdir -p /data/logs/kubernetes/kube-scheduler
[root@hdss-21 bin]# chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh
[root@hdss-22 bin]# cat   /opt/kubernetes/server/bin/kube-scheduler.sh
#!/bin/sh
./kube-scheduler \
  --leader-elect  \
  --log-dir /data/logs/kubernetes/kube-scheduler \
  --master http://127.0.0.1:8080 \
  --v 2
[root@hdss-22 bin]#  mkdir -p /data/logs/kubernetes/kube-scheduler
[root@hdss-22 bin]#  chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh
[root@hdss-21 bin]# cat  /etc/supervisord.d/kube-scheduler.ini
[program:kube-scheduler-7-21]
command=/opt/kubernetes/server/bin/kube-scheduler.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                               ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                     ; directory to cwd to before exec (def no cwd)
autostart=true                                                           ; start at supervisord start (default: true)
autorestart=true                                                         ; retstart at unexpected quit (default: true)
startsecs=30                                                             ; number of secs prog must stay running (def. 1)
startretries=3                                                           ; max # of serial start failures (default 3)
exitcodes=0,2                                                            ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                          ; signal used to kill process (default TERM)
stopwaitsecs=10                                                          ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                ; setuid to this UNIX account to run the program
redirect_stderr=true                                                     ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                             ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                 ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                              ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                              ; emit events on stdout writes (default false)
[root@hdss-22 bin]# cat   /etc/supervisord.d/kube-scheduler.ini
[program:kube-scheduler-7-22]
command=/opt/kubernetes/server/bin/kube-scheduler.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                               ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                     ; directory to cwd to before exec (def no cwd)
autostart=true                                                           ; start at supervisord start (default: true)
autorestart=true                                                         ; retstart at unexpected quit (default: true)
startsecs=30                                                             ; number of secs prog must stay running (def. 1)
startretries=3                                                           ; max # of serial start failures (default 3)
exitcodes=0,2                                                            ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                          ; signal used to kill process (default TERM)
stopwaitsecs=10                                                          ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                                ; setuid to this UNIX account to run the program
redirect_stderr=true                                                     ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                             ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                                 ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                              ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                              ; emit events on stdout writes (default false)
[root@hdss-21 bin]# supervisorctl update
kube-scheduler-7-21: added process group
[root@hdss-21 bin]# supervisorctl status
etcd-server-7-21                 RUNNING   pid 10233, uptime 2:01:49
kube-apiserver-7-21              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-21     RUNNING   pid 11008, uptime 0:07:59
kube-scheduler-7-21              STARTING  
[root@hdss-22 bin]# supervisorctl update
kube-scheduler-7-22: added process group
[root@hdss-22 bin]# supervisorctl status
etcd-server-7-22                 RUNNING   pid 803, uptime 23:57:50
kube-apiserver-7-22              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-22     RUNNING   pid 10477, uptime 0:07:04
kube-scheduler-7-22              STARTING  
[root@hdss-21 bin]# ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl
[root@hdss-21 bin]# which  kubectl  
/usr/bin/kubectl
[root@hdss-22 bin]# ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl
ln: failed to create symbolic link ‘/usr/bin/kubectl’: File exists
[root@hdss-22 bin]# which  kubectl 
/usr/bin/kubectl
[root@hdss-21 bin]#  kubectl get cs
The connection to the server localhost:8080 was refused - did you specify the right host or port?
[root@hdss-22 bin]#  kubectl get cs
The connection to the server localhost:8080 was refused - did you specify the right host or port?
[root@hdss-22 bin]# systemctl start   kube-apiserver    #解决问题
[root@hdss-22 bin]# systemctl status    kube-apiserver
[root@hdss-21 bin]# supervisorctl status
etcd-server-7-21                 RUNNING   pid 11222, uptime 0:00:34
kube-apiserver-7-21              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-21     RUNNING   pid 11220, uptime 0:00:34
kube-scheduler-7-21              RUNNING   pid 11221, uptime 0:00:34
[root@hdss-21 bin]#  kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {"health": "true"}   
在hdss-200上：
[root@hdss-200 certs]# vi  /opt/certs/kubelet-csr.json
[root@hdss-200 certs]# cat kubelet-csr.json
{
    "CN": "k8s-kubelet",
    "hosts": [
    "127.0.0.1",
    "192.168.26.10",
    "192.168.26.21",
    "192.168.26.22",
    "192.168.26.23",
    "192.168.26.24",
    "192.168.26.25",
    "192.168.26.26",
    "192.168.26.27",
    "192.168.26.28"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kubelet-csr.json | cfssl-json -bare kubelet
[root@hdss-200 certs]# ll
total 84
-rw-r--r-- 1 root root 1249 Jun 15 23:41 apiserver.csr
-rw-r--r-- 1 root root  562 Jun 15 23:41 apiserver-csr.json
-rw------- 1 root root 1675 Jun 15 23:41 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:41 apiserver.pem
-rw-r--r-- 1 root root  836 Jun 14 21:48 ca-config.json
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem
-rw-r--r-- 1 root root  993 Jun 15 23:29 client.csr
-rw-r--r-- 1 root root  392 Jun 15 23:24 client-csr.json
-rw------- 1 root root 1675 Jun 15 23:29 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:29 client.pem
-rw-r--r-- 1 root root 1062 Jun 14 21:58 etcd-peer.csr
-rw-r--r-- 1 root root  380 Jun 14 21:54 etcd-peer-csr.json
-rw------- 1 root root 1679 Jun 14 21:58 etcd-peer-key.pem
-rw-r--r-- 1 root root 1428 Jun 14 21:58 etcd-peer.pem
-rw-r--r-- 1 root root 1115 Jun 16 21:39 kubelet.csr
-rw-r--r-- 1 root root  488 Jun 16 21:35 kubelet-csr.json
-rw------- 1 root root 1679 Jun 16 21:39 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 16 21:39 kubelet.pem
在hdss-21/22上：
[root@hdss-21 cert]# scp hdss-200:/opt/certs/kubelet.pem .
[root@hdss-21 cert]# scp hdss-200:/opt/certs/kubelet-key.pem  .
[root@hdss-21 cert]# ll
total 32
-rw------- 1 root root 1675 Jun 15 23:53 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:53 apiserver.pem
-rw------- 1 root root 1679 Jun 15 23:52 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 15 23:52 ca.pem
-rw------- 1 root root 1675 Jun 15 23:53 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:53 client.pem
-rw------- 1 root root 1679 Jun 16 21:42 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 16 21:42 kubelet.pem
[root@hdss-21 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss-21 conf]# ls
audit.yaml
[root@hdss-21 conf]# kubectl config set-cluster myk8s \
>   --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \
>   --embed-certs=true \
>   --server=https://192.168.26.10:7443 \
>   --kubeconfig=kubelet.kubeconfig
Cluster "myk8s" set.
[root@hdss-21 conf]# kubectl config set-credentials k8s-node \
>   --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \
>   --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \
>   --embed-certs=true \
>   --kubeconfig=kubelet.kubeconfig 
User "k8s-node" set.
[root@hdss-21 conf]# kubectl config set-context myk8s-context \
>   --cluster=myk8s \
>   --user=k8s-node \
>   --kubeconfig=kubelet.kubeconfig
Context "myk8s-context" set.
[root@hdss-21 conf]# kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig
Switched to context "myk8s-context".
[root@hdss-22 bin]# cd   conf/
[root@hdss-22 conf]# ls
audit.yaml
[root@hdss-22 conf]# kubectl config set-cluster myk8s \
>   --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \
>   --embed-certs=true \
>   --server=https://192.168.26.10:7443 \
>   --kubeconfig=kubelet.kubeconfig
Cluster "myk8s" set.
[root@hdss-22 conf]#  kubectl config set-credentials k8s-node \
>   --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \
>   --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \
>   --embed-certs=true \
>   --kubeconfig=kubelet.kubeconfig 
User "k8s-node" set.
[root@hdss-22 conf]# kubectl config set-context myk8s-context \
>   --cluster=myk8s \
>   --user=k8s-node \
>   --kubeconfig=kubelet.kubeconfig
Context "myk8s-context" set.
[root@hdss-22 conf]# kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig
Switched to context "myk8s-context".
[root@hdss-22 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss-22 conf]# echo  "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR0RENDQXB5Z0F3SUJBZ0lVZm1SUlBDN1R3a0k4TGMzM0w1MFZpU21zN3M0d0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lERUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEN6QUpCZ05WQkFvVEFtOWtNUXd3Q2dZRFZRUUxFd052Y0hNeEVqQVFCZ05WQkFNVENVOXNaR0p2CmVVVmtkVEFlRncweU1EQTJNVFF3TkRJMk1EQmFGdzAwTURBMk1Ea3dOREkyTURCYU1HQXhDekFKQmdOVkJBWVQKQWtOT01SQXdEZ1lEVlFRSUV3ZENaV2xLYVc1bk1SQXdEZ1lEVlFRSEV3ZENaV2xLYVc1bk1Rc3dDUVlEVlFRSwpFd0p2WkRFTU1Bb0dBMVVFQ3hNRGIzQnpNUkl3RUFZRFZRUURFd2xQYkdSaWIzbEZaSFV3Z2dFaU1BMEdDU3FHClNJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURBSm4zbkZFRXV5S0Z4aEtkeVNoOGUybTNlNDRDYVJ1TTMKcnhKdmw0b1gxWGcxaUJsTEFROFhLLy9qbHFLNkFSeGJxSEVlYlpxL0J5dUlsaEkzQ3d4TzZQT3Z3cS9DMHpMUApCUXB6aUZFajhWVWFaSUV1ZFdJV1VIREgwa3A0MGNFRlc2SFlTbjVOTHFWYTVZR0JTc2hHTG4yelBaRWJtK2M3CkxyS2Y2YnNqb002RXRRbU53d01OazhwQzRSV2ZDRXpFSVNNaVphbXN2WG1DcUMxV2d3NmVwR0lBbEV3elFsYVMKdmlaV3M5ZGRoYVFYZU1UTXVnM095MDhsV3YvZ2Z6V2Fpc3BpaStEVzlsS1Y3MzFBanQxVW9VUzR5OTEycWowcQpwV2RUTSt5YnBiSGV5UXE5SkZQU1l2WUx1VlVMejFWNVFYR3BoU3ZQaHZBZjh3bWlsemVmQWdNQkFBR2paakJrCk1BNEdBMVVkRHdFQi93UUVBd0lCQmpBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQ01CMEdBMVVkRGdRV0JCU3EKNEFsYmdlaVV2OXRjNmdpbFBlWkVZQ1NsMkRBZkJnTlZIU01FR0RBV2dCU3E0QWxiZ2VpVXY5dGM2Z2lsUGVaRQpZQ1NsMkRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQXBSU1lIdkNVbVI4MkdoNXE4UTZud1YxQ2x1SVJzTTJaCjJlSW1vUTEzMmNWR056MUk3bEtFMFR1NVdKaXd0N1MzVndRUitPbU0rVHpNMVNrWkE1T2Rwd0Jrclo3ajNqSFAKaGdkS0lnSmJsbW9IUHE2bjNma01UT2ZHUHFIUERxWlYvOUhKNHNZekRsWVZ5UFRoS2pZdWI5Wi9yTUZjSjZTdQpoUStwQ0h2c010czZGdVIveFNDZkx6aUdNdStWa0lZdDFNVHVNcXVNU056VnAwSDZFeStwNzNQOXVnNEJvTkl2Ck5veTBxL2JTZVpzWkw5anE1WlF0UVl6TGdPOGlqS3owS0UxR2tsbjE1UG1PTVZabmMxenhWOHhmUE1RRUUraVUKQWdLV1BMZ3J6WnNhNW55elNmRWJXWHdsajdtOEpHcytrVm1LbTlQSXdSVnRGeTlSdEk5bFlBPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="|base64  -d
-----BEGIN CERTIFICATE-----
MIIDtDCCApygAwIBAgIUfmRRPC7TwkI8Lc33L50ViSms7s4wDQYJKoZIhvcNAQEL
BQAwYDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl
aUppbmcxCzAJBgNVBAoTAm9kMQwwCgYDVQQLEwNvcHMxEjAQBgNVBAMTCU9sZGJv
eUVkdTAeFw0yMDA2MTQwNDI2MDBaFw00MDA2MDkwNDI2MDBaMGAxCzAJBgNVBAYT
AkNOMRAwDgYDVQQIEwdCZWlKaW5nMRAwDgYDVQQHEwdCZWlKaW5nMQswCQYDVQQK
EwJvZDEMMAoGA1UECxMDb3BzMRIwEAYDVQQDEwlPbGRib3lFZHUwggEiMA0GCSqG
SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAJn3nFEEuyKFxhKdySh8e2m3e44CaRuM3
rxJvl4oX1Xg1iBlLAQ8XK//jlqK6ARxbqHEebZq/ByuIlhI3CwxO6POvwq/C0zLP
BQpziFEj8VUaZIEudWIWUHDH0kp40cEFW6HYSn5NLqVa5YGBSshGLn2zPZEbm+c7
LrKf6bsjoM6EtQmNwwMNk8pC4RWfCEzEISMiZamsvXmCqC1Wgw6epGIAlEwzQlaS
viZWs9ddhaQXeMTMug3Oy08lWv/gfzWaispii+DW9lKV731Ajt1UoUS4y912qj0q
pWdTM+ybpbHeyQq9JFPSYvYLuVULz1V5QXGphSvPhvAf8wmilzefAgMBAAGjZjBk
MA4GA1UdDwEB/wQEAwIBBjASBgNVHRMBAf8ECDAGAQH/AgECMB0GA1UdDgQWBBSq
4AlbgeiUv9tc6gilPeZEYCSl2DAfBgNVHSMEGDAWgBSq4AlbgeiUv9tc6gilPeZE
YCSl2DANBgkqhkiG9w0BAQsFAAOCAQEApRSYHvCUmR82Gh5q8Q6nwV1CluIRsM2Z
2eImoQ132cVGNz1I7lKE0Tu5WJiwt7S3VwQR+OmM+TzM1SkZA5OdpwBkrZ7j3jHP    #这个就是ca.pem证书
hgdKIgJblmoHPq6n3fkMTOfGPqHPDqZV/9HJ4sYzDlYVyPThKjYub9Z/rMFcJ6Su
hQ+pCHvsMts6FuR/xSCfLziGMu+VkIYt1MTuMquMSNzVp0H6Ey+p73P9ug4BoNIv
Noy0q/bSeZsZL9jq5ZQtQYzLgO8ijKz0KE1Gkln15PmOMVZnc1zxV8xfPMQEE+iU
AgKWPLgrzZsa5nyzSfEbWXwlj7m8JGs+kVmKm9PIwRVtFy9RtI9lYA==
-----END CERTIFICATE-----
[root@hdss-22 conf]# 
[root@hdss-200 certs]# ll
total 84
-rw-r--r-- 1 root root 1249 Jun 15 23:41 apiserver.csr
-rw-r--r-- 1 root root  562 Jun 15 23:41 apiserver-csr.json
-rw------- 1 root root 1675 Jun 15 23:41 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:41 apiserver.pem
-rw-r--r-- 1 root root  836 Jun 14 21:48 ca-config.json
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem
-rw-r--r-- 1 root root  993 Jun 15 23:29 client.csr
-rw-r--r-- 1 root root  392 Jun 15 23:24 client-csr.json
-rw------- 1 root root 1675 Jun 15 23:29 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:29 client.pem
-rw-r--r-- 1 root root 1062 Jun 14 21:58 etcd-peer.csr
-rw-r--r-- 1 root root  380 Jun 14 21:54 etcd-peer-csr.json
-rw------- 1 root root 1679 Jun 14 21:58 etcd-peer-key.pem
-rw-r--r-- 1 root root 1428 Jun 14 21:58 etcd-peer.pem
-rw-r--r-- 1 root root 1115 Jun 16 21:39 kubelet.csr
-rw-r--r-- 1 root root  488 Jun 16 21:35 kubelet-csr.json
-rw------- 1 root root 1679 Jun 16 21:39 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 16 21:39 kubelet.pem
[root@hdss-200 certs]# cat  ca.pem 
-----BEGIN CERTIFICATE-----
MIIDtDCCApygAwIBAgIUfmRRPC7TwkI8Lc33L50ViSms7s4wDQYJKoZIhvcNAQEL
BQAwYDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl
aUppbmcxCzAJBgNVBAoTAm9kMQwwCgYDVQQLEwNvcHMxEjAQBgNVBAMTCU9sZGJv
eUVkdTAeFw0yMDA2MTQwNDI2MDBaFw00MDA2MDkwNDI2MDBaMGAxCzAJBgNVBAYT
AkNOMRAwDgYDVQQIEwdCZWlKaW5nMRAwDgYDVQQHEwdCZWlKaW5nMQswCQYDVQQK
EwJvZDEMMAoGA1UECxMDb3BzMRIwEAYDVQQDEwlPbGRib3lFZHUwggEiMA0GCSqG
SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDAJn3nFEEuyKFxhKdySh8e2m3e44CaRuM3
rxJvl4oX1Xg1iBlLAQ8XK//jlqK6ARxbqHEebZq/ByuIlhI3CwxO6POvwq/C0zLP
BQpziFEj8VUaZIEudWIWUHDH0kp40cEFW6HYSn5NLqVa5YGBSshGLn2zPZEbm+c7
LrKf6bsjoM6EtQmNwwMNk8pC4RWfCEzEISMiZamsvXmCqC1Wgw6epGIAlEwzQlaS
viZWs9ddhaQXeMTMug3Oy08lWv/gfzWaispii+DW9lKV731Ajt1UoUS4y912qj0q
pWdTM+ybpbHeyQq9JFPSYvYLuVULz1V5QXGphSvPhvAf8wmilzefAgMBAAGjZjBk
MA4GA1UdDwEB/wQEAwIBBjASBgNVHRMBAf8ECDAGAQH/AgECMB0GA1UdDgQWBBSq
4AlbgeiUv9tc6gilPeZEYCSl2DAfBgNVHSMEGDAWgBSq4AlbgeiUv9tc6gilPeZE
YCSl2DANBgkqhkiG9w0BAQsFAAOCAQEApRSYHvCUmR82Gh5q8Q6nwV1CluIRsM2Z
2eImoQ132cVGNz1I7lKE0Tu5WJiwt7S3VwQR+OmM+TzM1SkZA5OdpwBkrZ7j3jHP
hgdKIgJblmoHPq6n3fkMTOfGPqHPDqZV/9HJ4sYzDlYVyPThKjYub9Z/rMFcJ6Su
hQ+pCHvsMts6FuR/xSCfLziGMu+VkIYt1MTuMquMSNzVp0H6Ey+p73P9ug4BoNIv
Noy0q/bSeZsZL9jq5ZQtQYzLgO8ijKz0KE1Gkln15PmOMVZnc1zxV8xfPMQEE+iU
AgKWPLgrzZsa5nyzSfEbWXwlj7m8JGs+kVmKm9PIwRVtFy9RtI9lYA==
-----END CERTIFICATE-----
[root@hdss-200 certs]# pwd
/opt/certs
[root@hdss-21 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss-21 conf]# cat  k8s-node.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: k8s-node
[root@hdss-21 conf]# ls
audit.yaml  k8s-node.yaml  kubelet.kubeconfig
[root@hdss-21 conf]# pwd
/opt/kubernetes/server/bin/conf
[root@hdss-21 conf]#  kubectl create -f k8s-node.yaml  
clusterrolebinding "k8s-node" created
[root@hdss-21 conf]# kubectl get clusterrolebinding k8s-node -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: 2020-06-18T15:06:52Z
  name: k8s-node
  resourceVersion: "750"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/k8s-node
  uid: e5a3b6e0-9f5a-4c17-a0a2-23a0b845c009
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: k8s-node
[root@hdss-21 conf]# kubectl create -f k8s-node.yaml   #只要在21这台机器上敲这个命令
[root@hdss-21 conf]# kubectl get clusterrolebinding k8s-node -o yaml
############
排错:
cat   /data/logs/kubernetes/kube-apiserver/apiserver.stdout.log看日志
[root@hdss-21 bin]# netstat -tnlp  |  grep  :8080
tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      28862/kube-apiserve 
[root@hdss-21 bin]# kill  28862
[root@hdss-21 bin]# ps -ef|grep api-server
root      29909    930  0 22:57 pts/0    00:00:00 grep --color=auto api-server
[root@hdss-21 bin]#  supervisorctl  status  
etcd-server-7-21                 RUNNING   pid 28701, uptime 1:31:09
kube-apiserver-7-21              FATAL     Exited too quickly (process log may have details)
kube-controller-manager-7-21     RUNNING   pid 28813, uptime 1:25:45
kube-scheduler-7-21              RUNNING   pid 28700, uptime 1:31:09
[root@hdss-21 bin]#  supervisorctl  start  kube-apiserver-7-21 
kube-apiserver-7-21: started
[root@hdss-21 bin]#  supervisorctl  status  
etcd-server-7-21                 RUNNING   pid 28701, uptime 1:32:13
kube-apiserver-7-21              RUNNING   pid 29920, uptime 0:00:51
kube-controller-manager-7-21     RUNNING   pid 28813, uptime 1:26:49
kube-scheduler-7-21              RUNNING   pid 28700, uptime 1:32:13
[root@hdss-21 conf]# kubectl  get  cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-2               Healthy   {"health": "true"}   
etcd-1               Healthy   {"health": "true"}   
etcd-0               Healthy   {"health": "true"}   
 1009  docker start $(docker ps -a | awk '{ print $1}' | tail -n +2)
 1010  vim  /opt/harbor-v1.8.3/docker-compose.yml
 1011  systemctl restart docker
 1012  docker-compose up -d 
 1013  docker-compose restart
 1014  docker  ps  -a
 1015  docker  stop  33a57649c615 
 1016  docker  rm  -f   33a57649c615 
 1017  docker  ps  -a
 1018  docker-compose restart
 1019  docker  ps  -a
 1020  docker  stop  e344141b36cb   
 1021  docker  rm  -f  e344141b36cb   
 1022  docker  ps  -a
 1023  docker-compose restart
 1024  docker  stop  df30c02f418
 1025  docker rm  -f   df30c02f418
###################
在hdss-200上:
[root@hdss-200 certs]# docker pull  kubernetes/pause
[root@hdss-200 certs]# docker images|grep  pause
kubernetes/pause                latest                     f9d5de079539        5 years ago         240kB
[root@hdss-200 certs]# docker  tag  f9d5de079539   harbor.od.com/public/pause:latest
[root@hdss-200 certs]# docker  push   harbor.od.com/public/pause:latest
[root@hdss-22 bin]# cat   /opt/kubernetes/server/bin/kubelet.sh
#!/bin/sh
./kubelet \
  --anonymous-auth=false \
  --cgroup-driver systemd \
  --cluster-dns 192.168.0.2 \
  --cluster-domain cluster.local \
  --runtime-cgroups=/systemd/system.slice \
  --kubelet-cgroups=/systemd/system.slice \
  --fail-swap-on="false" \
  --client-ca-file ./cert/ca.pem \
  --tls-cert-file ./cert/kubelet.pem \
  --tls-private-key-file ./cert/kubelet-key.pem \
  --hostname-override hdss-22.host.com \
  --image-gc-high-threshold 20 \
  --image-gc-low-threshold 10 \
  --kubeconfig ./conf/kubelet.kubeconfig \
  --log-dir /data/logs/kubernetes/kube-kubelet \
  --pod-infra-container-image harbor.od.com/public/pause:latest \
  --root-dir /data/kubelet
[root@hdss-22 bin]# chmod  +x  kubelet.sh 
[root@hdss-22 bin]# mkdir  -p  /data/logs/kubernetes/kube-kubelet    /data/kubelet
[root@hdss-21 bin]# cat  kubelet.sh 
#!/bin/sh
./kubelet \
  --anonymous-auth=false \
  --cgroup-driver systemd \
  --cluster-dns 192.168.0.2 \
  --cluster-domain cluster.local \
  --runtime-cgroups=/systemd/system.slice \
  --kubelet-cgroups=/systemd/system.slice \
  --fail-swap-on="false" \
  --client-ca-file ./cert/ca.pem \
  --tls-cert-file ./cert/kubelet.pem \
  --tls-private-key-file ./cert/kubelet-key.pem \
  --hostname-override hdss-21.host.com \
  --image-gc-high-threshold 20 \
  --image-gc-low-threshold 10 \
  --kubeconfig ./conf/kubelet.kubeconfig \
  --log-dir /data/logs/kubernetes/kube-kubelet \
  --pod-infra-container-image harbor.od.com/public/pause:latest \
  --root-dir /data/kubelet
[root@hdss-21 bin]# chmod +x  kubelet.sh 
[root@hdss-21 bin]# mkdir  -p  /data/logs/kubernetes/kube-kubelet    /data/kubelet
[root@hdss-21 bin]# cat   /etc/supervisord.d/kube-kubelet.ini
[program:kube-kubelet-7-21]
command=/opt/kubernetes/server/bin/kubelet.sh     ; the program (relative uses PATH, can take args)
numprocs=1                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin              ; directory to cwd to before exec (def no cwd)
autostart=true                                    ; start at supervisord start (default: true)
autorestart=true                                ; retstart at unexpected quit (default: true)
startsecs=30                                      ; number of secs prog must stay running (def. 1)
startretries=3                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log   ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                       ; emit events on stdout writes (default false)
[root@hdss-22 bin]# cat   /etc/supervisord.d/kube-kubelet.ini
[program:kube-kubelet-7-22]
command=/opt/kubernetes/server/bin/kubelet.sh     ; the program (relative uses PATH, can take args)
numprocs=1                                        ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin              ; directory to cwd to before exec (def no cwd)
autostart=true                                    ; start at supervisord start (default: true)
autorestart=true                                ; retstart at unexpected quit (default: true)
startsecs=30                                      ; number of secs prog must stay running (def. 1)
startretries=3                                    ; max # of serial start failures (default 3)
exitcodes=0,2                                     ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                   ; signal used to kill process (default TERM)
stopwaitsecs=10                                   ; max num secs to wait b4 SIGKILL (default 10)
user=root                                         ; setuid to this UNIX account to run the program
redirect_stderr=true                              ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log   ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                      ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                          ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                       ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                       ; emit events on stdout writes (default false)
[root@hdss-21 bin]# supervisorctl  update 
kube-kubelet-7-21: added process group
[root@hdss-21 bin]# supervisorctl  status  
etcd-server-7-21                 RUNNING   pid 28701, uptime 22:42:22
kube-apiserver-7-21              RUNNING   pid 30075, uptime 21:02:58
kube-controller-manager-7-21     RUNNING   pid 30068, uptime 21:03:03
kube-kubelet-7-21                RUNNING   pid 38088, uptime 0:00:52
kube-scheduler-7-21              RUNNING   pid 30062, uptime 21:03:04
[root@hdss-21 bin]# cat  /etc/resolv.conf 
# Generated by NetworkManager
search host.com
nameserver 192.168.26.11
[root@hdss-21 bin]# tail   -fn  200    /data/logs/kubernetes/kube-kubelet/kubelet.stdout.log 

[root@hdss-22 bin]# supervisorctl  update 
kube-kubelet-7-22: added process group
[root@hdss-22 bin]# supervisorctl  status 
etcd-server-7-22                 RUNNING   pid 28035, uptime 22:42:17
kube-apiserver-7-22              RUNNING   pid 28940, uptime 21:09:23
kube-controller-manager-7-22     RUNNING   pid 28933, uptime 21:09:28
kube-kubelet-7-22                RUNNING   pid 36996, uptime 0:01:32
kube-scheduler-7-22              RUNNING   pid 28032, uptime 22:42:17
[root@hdss-22 bin]# cat  /etc/resolv.conf 
# Generated by NetworkManager
search host.com
nameserver 192.168.26.11
[root@hdss-21 bin]# kubectl  get nodes
NAME               STATUS    AGE
hdss-21.host.com   Ready     2m
hdss-22.host.com   Ready     3m
[root@hdss-21 bin]# kubectl label node hdss-21.host.com node-role.kubernetes.io/master=
node "hdss-21.host.com" labeled
[root@hdss-21 bin]# kubectl label node hdss-21.host.com node-role.kubernetes.io/node=
node "hdss-21.host.com" labeled
[root@hdss-21 bin]# kubectl get node --show-labels
NAME               STATUS    AGE       LABELS
hdss-21.host.com   Ready     11m       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=hdss-21.host.com,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/node=
hdss-22.host.com   Ready     11m       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=hdss-22.host.com,kubernetes.io/os=linux
[root@hdss-21 bin]# 
[root@hdss-21 bin]# kubectl label node hdss-22.host.com node-role.kubernetes.io/master=
node "hdss-22.host.com" labeled
[root@hdss-21 bin]# kubectl label node hdss-22.host.com node-role.kubernetes.io/node=
node "hdss-22.host.com" labeled
[root@hdss-21 bin]# kubectl get node --show-labels
NAME               STATUS    AGE       LABELS
hdss-21.host.com   Ready     12m       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=hdss-21.host.com,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/node=
hdss-22.host.com   Ready     13m       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=hdss-22.host.com,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/node=
[root@hdss-200 certs]# cat    kube-proxy-csr.json
{
    "CN": "system:kube-proxy",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
[root@hdss-200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client kube-proxy-csr.json |cfssl-json -bare kube-proxy-client
2020/06/19 20:33:54 [INFO] generate received request
2020/06/19 20:33:54 [INFO] received CSR
2020/06/19 20:33:54 [INFO] generating key: rsa-2048
2020/06/19 20:33:55 [INFO] encoded CSR
2020/06/19 20:33:55 [INFO] signed certificate with serial number 584121090790807149832141351741936230064836441407
2020/06/19 20:33:55 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
[root@hdss-200 certs]# ll
total 100
-rw-r--r-- 1 root root 1249 Jun 15 23:41 apiserver.csr
-rw-r--r-- 1 root root  562 Jun 15 23:41 apiserver-csr.json
-rw------- 1 root root 1675 Jun 15 23:41 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 15 23:41 apiserver.pem
-rw-r--r-- 1 root root  836 Jun 14 21:48 ca-config.json
-rw-r--r-- 1 root root  993 Jun 14 12:30 ca.csr
-rw-r--r-- 1 root root  261 Jun 14 12:24 ca-csr.json
-rw------- 1 root root 1679 Jun 14 12:30 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 14 12:30 ca.pem
-rw-r--r-- 1 root root  993 Jun 15 23:29 client.csr
-rw-r--r-- 1 root root  392 Jun 15 23:24 client-csr.json
-rw------- 1 root root 1675 Jun 15 23:29 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 15 23:29 client.pem
-rw-r--r-- 1 root root 1062 Jun 14 21:58 etcd-peer.csr
-rw-r--r-- 1 root root  380 Jun 14 21:54 etcd-peer-csr.json
-rw------- 1 root root 1679 Jun 14 21:58 etcd-peer-key.pem
-rw-r--r-- 1 root root 1428 Jun 14 21:58 etcd-peer.pem
-rw-r--r-- 1 root root 1115 Jun 16 21:39 kubelet.csr
-rw-r--r-- 1 root root  488 Jun 16 21:35 kubelet-csr.json
-rw------- 1 root root 1679 Jun 16 21:39 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 16 21:39 kubelet.pem
-rw-r--r-- 1 root root 1005 Jun 19 20:33 kube-proxy-client.csr
-rw------- 1 root root 1675 Jun 19 20:33 kube-proxy-client-key.pem
-rw-r--r-- 1 root root 1375 Jun 19 20:33 kube-proxy-client.pem
-rw-r--r-- 1 root root  267 Jun 19 20:31 kube-proxy-csr.json
6套证书
在hdss-21/22上：
[root@hdss-21 bin]# pwd
/opt/kubernetes/server/bin
[root@hdss-21 cert]#  scp hdss-200:/opt/certs/kube-proxy-client-key.pem .
root@hdss-200's password: 
kube-proxy-client-key.pem                                                                                                                                    100% 1675   577.4KB/s   00:00    
[root@hdss-21 cert]# scp hdss-200:/opt/certs/kube-proxy-client.pem .
root@hdss-200's password: 
kube-proxy-client.pem                                                                                                                                        100% 1375   142.3KB/s   00:00    
[root@hdss-21 cert]# ll
total 40
-rw------- 1 root root 1675 Jun 18 20:35 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 18 20:35 apiserver.pem
-rw------- 1 root root 1679 Jun 18 20:35 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 18 20:34 ca.pem
-rw------- 1 root root 1675 Jun 18 20:35 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 18 20:35 client.pem
-rw------- 1 root root 1679 Jun 18 20:58 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 18 20:58 kubelet.pem
-rw------- 1 root root 1675 Jun 19 20:40 kube-proxy-client-key.pem
-rw-r--r-- 1 root root 1375 Jun 19 20:41 kube-proxy-client.pem
[root@hdss-22 cert]# scp hdss-200:/opt/certs/kube-proxy-client-key.pem .
root@hdss-200's password: 
kube-proxy-client-key.pem                                                                                                                                    100% 1675    70.9KB/s   00:00    
[root@hdss-22 cert]# scp hdss-200:/opt/certs/kube-proxy-client.pem .
root@hdss-200's password: 
kube-proxy-client.pem                                                                                                                                        100% 1375    31.8KB/s   00:00    
[root@hdss-22 cert]# ll
total 40
-rw------- 1 root root 1675 Jun 18 20:40 apiserver-key.pem
-rw-r--r-- 1 root root 1598 Jun 18 20:39 apiserver.pem
-rw------- 1 root root 1679 Jun 18 20:39 ca-key.pem
-rw-r--r-- 1 root root 1346 Jun 18 20:39 ca.pem
-rw------- 1 root root 1675 Jun 18 20:39 client-key.pem
-rw-r--r-- 1 root root 1363 Jun 18 20:39 client.pem
-rw------- 1 root root 1679 Jun 18 20:59 kubelet-key.pem
-rw-r--r-- 1 root root 1468 Jun 18 20:58 kubelet.pem
-rw------- 1 root root 1675 Jun 19 20:42 kube-proxy-client-key.pem
-rw-r--r-- 1 root root 1375 Jun 19 20:43 kube-proxy-client.pem
[root@hdss-21 conf]# kubectl config set-cluster myk8s \
>   --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \
>   --embed-certs=true \
>   --server=https://192.168.26.10:7443 \
>   --kubeconfig=kube-proxy.kubeconfig
Cluster "myk8s" set.
[root@hdss-21 conf]# ls
audit.yaml  k8s-node.yaml  kubelet.kubeconfig  kube-proxy.kubeconfig
[root@hdss-21 conf]# kubectl config set-credentials kube-proxy \
>   --client-certificate=/opt/kubernetes/server/bin/cert/kube-proxy-client.pem \
>   --client-key=/opt/kubernetes/server/bin/cert/kube-proxy-client-key.pem \
>   --embed-certs=true \
>   --kubeconfig=kube-proxy.kubeconfig
User "kube-proxy" set.
[root@hdss-21 conf]# kubectl config set-context myk8s-context \
>   --cluster=myk8s \
>   --user=kube-proxy \
>   --kubeconfig=kube-proxy.kubeconfig
Context "myk8s-context" set.
[root@hdss-21 conf]# kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig
Switched to context "myk8s-context".
[root@hdss-21 conf]# ll
total 24
-rw-r--r-- 1 root root 2230 Jun 18 20:38 audit.yaml
-rw-r--r-- 1 root root  258 Jun 18 22:08 k8s-node.yaml
-rw------- 1 root root 6199 Jun 18 21:08 kubelet.kubeconfig
-rw------- 1 root root 6219 Jun 19 20:47 kube-proxy.kubeconfig
[root@hdss-21 conf]# scp   kube-proxy.kubeconfig    hdss-22:/opt/kubernetes/server/bin/conf
[root@hdss-21 conf]# ll
total 24
-rw-r--r-- 1 root root 2230 Jun 18 20:38 audit.yaml
-rw-r--r-- 1 root root  258 Jun 18 22:08 k8s-node.yaml
-rw------- 1 root root 6199 Jun 18 21:08 kubelet.kubeconfig
-rw------- 1 root root 6219 Jun 19 20:47 kube-proxy.kubeconfig
[root@hdss-22 conf]# ll
total 20
-rw-r--r-- 1 root root 2230 Jun 18 20:42 audit.yaml
-rw------- 1 root root 6199 Jun 18 21:08 kubelet.kubeconfig
-rw------- 1 root root 6219 Jun 19 20:49 kube-proxy.kubeconfig
[root@hdss-21 ~]# lsmod |  grep  ip_vs
[root@hdss-21 ~]# vim  /root/ipvs.sh
[root@hdss-21 ~]# cat   /root/ipvs.sh
#!/bin/bash
ipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs"
for i in $(ls $ipvs_mods_dir|grep -o "^[^.]*")
do
  /sbin/modinfo -F filename $i &>/dev/null
  if [ $? -eq 0 ];then
    /sbin/modprobe $i
  fi
done
[root@hdss-21 ~]# chmod  +x  ipvs.sh 
[root@hdss-21 ~]# ls
anaconda-ks.cfg  ipvs.sh  kubernetes-server-linux-amd64.tar.gz  python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm
[root@hdss-21 ~]# ./ipvs.sh 
[root@hdss-21 ~]# lsmod |  grep  ip_vs
ip_vs_wrr              12697  0 
ip_vs_wlc              12519  0 
ip_vs_sh               12688  0 
ip_vs_sed              12519  0 
ip_vs_rr               12600  0 
ip_vs_pe_sip           12697  0 
nf_conntrack_sip       33860  1 ip_vs_pe_sip
ip_vs_nq               12516  0 
ip_vs_lc               12516  0 
ip_vs_lblcr            12922  0 
ip_vs_lblc             12819  0 
ip_vs_ftp              13079  0 
ip_vs_dh               12688  0 
ip_vs                 141092  24 ip_vs_dh,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sh,ip_vs_ftp,ip_vs_sed,ip_vs_wlc,ip_vs_wrr,ip_vs_pe_sip,ip_vs_lblcr,ip_vs_lblc
nf_nat                 26787  3 ip_vs_ftp,nf_nat_ipv4,nf_nat_masquerade_ipv4
nf_conntrack          133387  8 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_sip,nf_conntrack_ipv4
libcrc32c              12644  5 xfs,sctp,ip_vs,nf_nat,nf_conntrack
[root@hdss-22 ~]# cat   ipvs.sh
#!/bin/bash
ipvs_mods_dir="/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs"
for i in $(ls $ipvs_mods_dir|grep -o "^[^.]*")
do
  /sbin/modinfo -F filename $i &>/dev/null
  if [ $? -eq 0 ];then
    /sbin/modprobe $i
  fi
done

[root@hdss-22 ~]# ls
anaconda-ks.cfg  ipvs.sh  kubernetes-server-linux-amd64.tar.gz  python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm
[root@hdss-22 ~]# chmod  +x  ipvs.sh 
[root@hdss-22 ~]# ls
anaconda-ks.cfg  ipvs.sh  kubernetes-server-linux-amd64.tar.gz  python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm
[root@hdss-22 ~]# ./ipvs.sh 
[root@hdss-22 ~]# lsmod |  grep  ip_vs
ip_vs_wrr              12697  0 
ip_vs_wlc              12519  0 
ip_vs_sh               12688  0 
ip_vs_sed              12519  0 
ip_vs_rr               12600  0 
ip_vs_pe_sip           12697  0 
nf_conntrack_sip       33860  1 ip_vs_pe_sip
ip_vs_nq               12516  0 
ip_vs_lc               12516  0 
ip_vs_lblcr            12922  0 
ip_vs_lblc             12819  0 
ip_vs_ftp              13079  0 
ip_vs_dh               12688  0 
ip_vs                 141092  24 ip_vs_dh,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sh,ip_vs_ftp,ip_vs_sed,ip_vs_wlc,ip_vs_wrr,ip_vs_pe_sip,ip_vs_lblcr,ip_vs_lblc
nf_nat                 26787  3 ip_vs_ftp,nf_nat_ipv4,nf_nat_masquerade_ipv4
nf_conntrack          133387  8 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_sip,nf_conntrack_ipv4
libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack
[root@hdss-21 bin]# cat  /opt/kubernetes/server/bin/kube-proxy.sh
#!/bin/sh
./kube-proxy \
  --cluster-cidr 172.7.0.0/16 \
  --hostname-override hdss-21.host.com \
  --proxy-mode=ipvs \
  --ipvs-scheduler=nq \
  --kubeconfig ./conf/kube-proxy.kubeconfig
[root@hdss-21 bin]# chmod +x /opt/kubernetes/server/bin/kube-proxy.sh
[root@hdss-22 bin]# cat   kube-proxy.sh
#!/bin/sh
./kube-proxy \
  --cluster-cidr 172.7.0.0/16 \
  --hostname-override hdss-22.host.com \
  --proxy-mode=ipvs \
  --ipvs-scheduler=nq \
  --kubeconfig ./conf/kube-proxy.kubeconfig
[root@hdss-22 bin]# chmod +x   kube-proxy.sh 
[root@hdss-21 bin]#  mkdir -p /data/logs/kubernetes/kube-proxy
[root@hdss-22 bin]#  mkdir -p /data/logs/kubernetes/kube-proxy
[root@hdss-21 bin]# cat   /etc/supervisord.d/kube-proxy.ini
[program:kube-proxy-7-21]
command=/opt/kubernetes/server/bin/kube-proxy.sh                     ; the program (relative uses PATH, can take args)
numprocs=1                                                           ; number of processes copies to start (def 1)
directory=/opt/kubernetes/server/bin                                 ; directory to cwd to before exec (def no cwd)
autostart=true                                                       ; start at supervisord start (default: true)
autorestart=true                                                     ; retstart at unexpected quit (default: true)
startsecs=30                                                         ; number of secs prog must stay running (def. 1)
startretries=3                                                       ; max # of serial start failures (default 3)
exitcodes=0,2                                                        ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                                      ; signal used to kill process (default TERM)
stopwaitsecs=10                                                      ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                            ; setuid to this UNIX account to run the program
redirect_stderr=true                                                 ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/kubernetes/kube-proxy/proxy.stdout.log     ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                         ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                             ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                          ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                          ; emit events on stdout writes (default false)
[root@hdss-21 bin]#  supervisorctl update
kube-proxy-7-21: added process group
[root@hdss-21 bin]#  supervisorctl status 
etcd-server-7-21                 RUNNING   pid 28701, uptime 23:47:30
kube-apiserver-7-21              RUNNING   pid 30075, uptime 22:08:06
kube-controller-manager-7-21     RUNNING   pid 30068, uptime 22:08:11
kube-kubelet-7-21                RUNNING   pid 38088, uptime 1:06:00
kube-proxy-7-21                  STARTING  
kube-scheduler-7-21              RUNNING   pid 30062, uptime 22:08:12
[root@hdss-21 bin]# tail  -fn  200  /data/logs/kubernetes/kube-proxy/proxy.stdout.log  
W0619 21:13:52.575930   49311 server.go:216] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP.
I0619 21:13:53.826368   49311 server_others.go:170] Using ipvs Proxier.
[root@hdss-21 bin]# yum  install  ipvsadm  -y  
[root@hdss-22 bin]# yum  install  ipvsadm  -y  
[root@hdss-21 bin]# ipvsadm  -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 192.168.26.21:6443           Masq    1      0          0         
  -> 192.168.26.22:6443           Masq    1      0          0        
[root@hdss-22 bin]# ipvsadm  -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 192.168.26.21:6443           Masq    1      0          0         
  -> 192.168.26.22:6443           Masq    1      0          0    
[root@hdss-22 bin]# kubectl  get svc
NAME         CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   192.168.0.1   <none>        443/TCP   1d
#############排错#####################
[root@hdss-21 bin]# tail  -fn  200  /data/logs/kubernetes/kube-kubelet/kubelet.stdout.log  
[root@hdss-21 bin]# systemctl  restart    docker
[root@hdss-22 bin]# supervisorctl status 
etcd-server-7-22                 RUNNING   pid 800, uptime 0:51:52
kube-apiserver-7-22              RUNNING   pid 789, uptime 0:51:52
kube-controller-manager-7-22     RUNNING   pid 794, uptime 0:51:52
kube-kubelet-7-22                FATAL     Exited too quickly (process log may have details)
kube-proxy-7-22                  RUNNING   pid 790, uptime 0:51:52
kube-scheduler-7-22              RUNNING   pid 804, uptime 0:51:52
[root@hdss-22 bin]# supervisorctl  restart  kube-kubelet-7-22  
[root@hdss-22 bin]# supervisorctl status 
etcd-server-7-22                 RUNNING   pid 800, uptime 0:52:40
kube-apiserver-7-22              RUNNING   pid 789, uptime 0:52:40
kube-controller-manager-7-22     RUNNING   pid 794, uptime 0:52:40
kube-kubelet-7-22                RUNNING   pid 6194, uptime 0:00:36
kube-proxy-7-22                  RUNNING   pid 790, uptime 0:52:40
kube-scheduler-7-22              RUNNING   pid 804, uptime 0:52:40
[root@hdss-21 bin]# kubectl  get  cs
NAME                 STATUS    MESSAGE              ERROR
etcd-0               Healthy   {"health": "true"}   
etcd-2               Healthy   {"health": "true"}   
etcd-1               Healthy   {"health": "true"}   
scheduler            Healthy   ok                   
controller-manager   Healthy   ok   
############################################
在hdss-21上：
[root@hdss-21 ~]# cat  /etc/resolv.conf 
# Generated by NetworkManager
search host.com
nameserver 192.168.26.11
[root@hdss-22 bin]# cat  /etc/resolv.conf 
# Generated by NetworkManager
search host.com
nameserver 192.168.26.11
[root@hdss-21 bin]# cat  /root/nginx-ds.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ds
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: harbor.od.com/public/nginx:v1.7.9
        ports:
        - containerPort: 80
[root@hdss-21 ~]# kubectl delete  -f nginx-ds.yaml 
daemonset.extensions "nginx-ds" deleted
[root@hdss-21 ~]# kubectl create -f nginx-ds.yaml 
daemonset.extensions/nginx-ds created
[root@hdss-21 ~]# kubectl  get pods
NAME             READY     STATUS              RESTARTS   AGE
nginx-ds-4djng   0/1       ContainerCreating   0          2m
nginx-ds-kxwhw   0/1       ContainerCreating   0          2m
[root@hdss-22 ~]# kubectl  get  pods  -o  wide
NAME             READY     STATUS        RESTARTS   AGE       IP        NODE
nginx-ds-n59kc   0/1       Terminating   0          17h       <none>    hdss-22.host.com
[root@hdss-22 ~]# kubectl delete pods  nginx-ds-n59kc   --grace-period=0 --force
[root@hdss-22 bin]#  kubectl  get pods  -o  wide 
NAME             READY     STATUS              RESTARTS   AGE       IP           NODE
nginx-ds-kfx8c   0/1       ImagePullBackOff    0          7m        172.7.21.2   hdss-21.host.com
nginx-ds-m6dqt   0/1       ContainerCreating   0          7m        <none>       hdss-22.host.com
[root@hdss-21 ~]#   kubectl  describe  pod  nginx-ds-p6mp4 
[root@hdss-22 bin]# curl  172.7.21.2
[root@hdss-22 bin]# kubectl   get  cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
etcd-0               Healthy   {"health": "true"}   
etcd-1               Healthy   {"health": "true"}   
etcd-2               Healthy   {"health": "true"}   
scheduler            Healthy   ok   
[root@hdss-22 bin]# kubectl   get  node
NAME               STATUS    AGE
hdss-21.host.com   Ready     20h
hdss-22.host.com   Ready     20h
[root@hdss-22 bin]# kubectl   get  pods
NAME             READY     STATUS              RESTARTS   AGE
nginx-ds-kfx8c   0/1       ImagePullBackOff    0          18m
nginx-ds-m6dqt   0/1       ContainerCreating   0          18m
https://www.cnblogs.com/wangchaolinux/p/11875124.html参考文档
https://www.cnblogs.com/wangchaolinux/p/11921202.html参考文档
####################################################
解决报错：
[root@hdss-22 bin]# kubectl   get  pods
NAME             READY     STATUS              RESTARTS   AGE
nginx-ds-kfx8c   0/1       ImagePullBackOff    0          18m
nginx-ds-m6dqt   0/1       ContainerCreating   0          18m
[root@hdss-22/21 cert]# docker  pull    harbor.od.com/public/nginx:v1.7.9
v1.7.9: Pulling from public/nginx
4f4fb700ef54: Pull complete 
8c71e11b018e: Pull complete 
32a497444d35: Pull complete 
8f6a37a5f8b6: Pull complete 
b0568fa3217a: Pull complete 
2188268d060d: Pull complete 
5af4b0ff64b0: Pull complete 
Digest: sha256:b1f5935eb2e9e2ae89c0b3e2e148c19068d91ca502e857052f14db230443e4c2
Status: Downloaded newer image for harbor.od.com/public/nginx:v1.7.9
[root@hdss-22/21 bin]#  docker  pull    harbor.od.com/public/pause
Using default tag: latest
latest: Pulling from public/pause
4f4fb700ef54: Pull complete 
b9c8ec465f6b: Pull complete 
Digest: sha256:b31bfb4d0213f254d361e0079deaaebefa4f82ba7aa76ef82e90b4935ad5b105
Status: Downloaded newer image for harbor.od.com/public/pause:latest
harbor.od.com/public/pause:latest
[root@hdss-22 ~]# kubectl delete pods  nginx-ds-n59kc   --grace-period=0 --force
[root@hdss-21 ~]#  kubectl   get pods  -o  wide 
NAME             READY   STATUS    RESTARTS   AGE   IP           NODE               NOMINATED NODE   READINESS GATES
nginx-ds-2g87s   1/1     Running   0          23m   172.7.22.3   hdss-22.host.com   <none>           <none>
nginx-ds-5lhbz   1/1     Running   0          16m   172.7.21.2   hdss-21.host.com   <none>           <none>
######################################################
[root@hdss-21 ~]# kubectl get namespace
NAME              STATUS   AGE
default           Active   2d13h
kube-node-lease   Active   2d13h
kube-public       Active   2d13h
kube-system       Active   2d13h
[root@hdss-21 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   2d13h
kube-node-lease   Active   2d13h
kube-public       Active   2d13h
kube-system       Active   2d13h
[root@hdss-21 ~]# kubectl get all -n default
NAME                 READY   STATUS              RESTARTS   AGE
pod/nginx-ds-bmr72   0/1     ImagePullBackOff    0          15h
pod/nginx-ds-n59kc   0/1     ContainerCreating   0          15h
NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   192.168.0.1   <none>        443/TCP   2d13h
NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/nginx-ds   0         0         0       0            0           <none>          15h

[root@hdss-21 ~]# kubectl create namespace app  
namespace/app created

[root@hdss-21 ~]# kubectl get ns
NAME              STATUS   AGE
app               Active   6s
default           Active   2d13h
kube-node-lease   Active   2d13h
kube-public       Active   2d13h
kube-system       Active   2d13h

[root@hdss-21 ~]# kubectl  delete  ns  app 
namespace "app" deleted

[root@hdss-21 ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   2d13h
kube-node-lease   Active   2d13h
kube-public       Active   2d13h
kube-system       Active   2d13h

[root@hdss-21 ~]# kubectl get all -n default  #与下面一样

[root@hdss-21 ~]# kubectl get all

[root@hdss-21 ~]# kubectl  create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public
deployment.apps/nginx-dp created


[root@hdss-21 ~]# kubectl  get deployment -n kube-public
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
nginx-dp   0/1     0            0           34s

[root@hdss61-21 ~]# kubectl  get deployment -n kube-public  
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-db   1         1         1            1           1d

[root@hdss-21 ~]#  kubectl  get pods  -n kube-public
NAME                        READY   STATUS         RESTARTS   AGE
nginx-dp-5dfc689474-kn4j5   0/1     ErrImagePull   0          4h24m

[root@hdss61-21 ~]#  kubectl  get pods  -n kube-public  
NAME                       READY     STATUS    RESTARTS   AGE
nginx-db-c67cdb84c-tt4lx   1/1       Running   0          1d


[root@hdss-21 ~]#  kubectl  get pods  -n kube-public  -o wide 
NAME                        READY   STATUS             RESTARTS   AGE     IP           NODE               NOMINATED NODE   READINESS GATES
nginx-dp-5dfc689474-kn4j5   0/1     ImagePullBackOff   0          4h25m   172.7.21.2   hdss-21.host.com   <none>           <none>

[root@hdss61-21 ~]# kubectl  get pods  -n kube-public  -o wide  
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
nginx-db-c67cdb84c-tt4lx   1/1       Running   0          1d        172.7.22.3   hdss61-22.host.com

[root@hdss61-21 ~]#  kubectl get pod -n kube-public -o wide  
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
nginx-db-c67cdb84c-tt4lx   1/1       Running   0          1d        172.7.22.3   hdss61-22.host.com


[root@hdss-21 ~]#  kubectl  get  deployment   -n kube-public  -o wide 
NAME       READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                              SELECTOR
nginx-dp   0/1     1            0           4h34m   nginx        harbor.od.com/public/nginx:v1.7.9   app=nginx-dp

[root@hdss61-21 ~]# kubectl  get  deployment   -n kube-public  -o wide  
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-db   1         1         1            1           1d


[root@hdss-21 ~]# kubectl describe deployment nginx-dp -n kube-public

[root@hdss-21 ~]#  kubectl   get pods
NAME             READY   STATUS              RESTARTS   AGE
nginx-ds-bc6wt   0/1     ContainerCreating   0          8m13s
nginx-ds-d2jsc   0/1     ImagePullBackOff    0          8m13s

[root@hdss61-21 ~]# kubectl get all -n default
NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
svc/kubernetes   192.168.0.1      <none>        443/TCP    3d
svc/nginx-ds     192.168.52.225   <none>        8080/TCP   1d

curl  192.168.52.225  


NAME                READY     STATUS    RESTARTS   AGE
po/nginx-ds-9kbmj   1/1       Running   0          1d
po/nginx-ds-ks6cp   1/1       Running   0          1d

[root@hdss-21 ~]#  kubectl   get  pods  -n  kube-public 
NAME                        READY   STATUS              RESTARTS   AGE
nginx-dp-5dfc689474-ndhxk   0/1     ContainerCreating   0          10m

[root@hdss-21 ~]# kubectl  exec  -ti  nginx-dp-5dfc689474-ndhxk  /bin/bash  -n  kube-public

[root@hdss-22 ~]# docker  ps  -a | grep  nginx-dp
5dfaaf336c76        84581e99d807                        "nginx -g 'daemon of…"   About an hour ago   Exited (255) 32 minutes ago                       k8s_nginx_nginx-dp-5dfc689474-ndhxk_kube-public_85c15723-c5be-457c-8708-a38eb9162fd7_0
95a239d8001f        harbor.od.com/public/pause:latest   "/pause"                 About an hour ago   Exited (255) 32 minutes ago                       k8s_POD_nginx-dp-5dfc689474-ndhxk_kube-
[root@hdss-22 ~]# docker exec -ti  5dfa bash 

[root@hdss-22 bin]# kubectl delete pod nginx-dp-5dfc689474-rv8fz  -n kube-public
pod "nginx-dp-5dfc689474-rv8fz" deleted

[root@hdss-22 bin]#  kubectl  get pods  -n  kube-public 
NAME                        READY     STATUS        RESTARTS   AGE
nginx-dp-5dfc689474-68nxh   0/1       Pending       0          2s
nginx-dp-5dfc689474-ndhxk   1/1       Terminating   0          1h

[root@hdss-22 bin]# kubectl  get  pods -n kube-public -o  wide
NAME                        READY     STATUS        RESTARTS   AGE       IP           NODE
nginx-dp-5dfc689474-68nxh   0/1       Pending       0          48s       <none>       
nginx-dp-5dfc689474-ndhxk   1/1       Terminating   0          1h        172.7.22.2   hdss-22.host.com

[root@hdss-21 ~]# kubectl delete pod nginx-dp-5dfc689474-rv8fz  -n kube-public --force --grace-period=0
 
[root@hdss-21 ~]# kubectl expose deployment nginx-dp --port=80 -n kube-public
service/nginx-dp exposed

[root@hdss-21 ~]# kubectl  get all -n kube-public 

[root@hdss-21 ~]# kubectl  get pods  -n kube-public  -o  wide 

[root@hdss-22 bin]# kubectl  delete deploy nginx-dp -n  kube-public 
deployment "nginx-dp" deleted

[root@hdss-22 bin]# kubectl  get all -n kube-public 
NAME                           READY     STATUS        RESTARTS   AGE
po/nginx-dp-5dfc689474-ndhxk   1/1       Terminating   0          2h

[root@hdss-22 bin]# kubectl  delete   po/nginx-dp-5dfc689474-ndhxk  -n  kube-public   --grace-period=0 --force 
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "nginx-dp-5dfc689474-ndhxk" deleted

[root@hdss-22 bin]# kubectl  get all -n kube-public 
No resources found.

[root@hdss61-22 ~]# ipvsadm -Ln  
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 192.168.61.119:6443          Masq    1      0          0         
  -> 192.168.61.120:6443          Masq    1      0          0         
TCP  192.168.52.225:8080 nq
  -> 172.7.21.2:80                Masq    1      0          0         
  -> 172.7.22.2:80                Masq    1      0          0         
TCP  192.168.220.66:80 nq
  -> 172.7.22.3:80                Masq    1      0          0   

[root@hdss61-21 ~]# kubectl describe svc nginx-db  -n kube-public
Name:			nginx-db
Namespace:		kube-public
Labels:			app=nginx-db
Selector:		app=nginx-db
Type:			ClusterIP
IP:			192.168.220.66
Port:			<unset>	80/TCP
Endpoints:		172.7.22.3:80
Session Affinity:	None
No events.

[root@hdss61-21 ~]# kubectl scale deployment nginx-db  --replicas=2  -n kube-public  
deployment "nginx-db" scaled

[root@hdss61-21 ~]# ipvsadm -Ln  
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 192.168.61.119:6443          Masq    1      0          0         
  -> 192.168.61.120:6443          Masq    1      0          0         
TCP  192.168.52.225:8080 nq
  -> 172.7.21.2:80                Masq    1      0          0         
  -> 172.7.22.2:80                Masq    1      0          0         
TCP  192.168.220.66:80 nq

kubectl命令大全：
* --help
* http://doce.kubernetes.org.cn

[root@hdss61-21 ~]#  kubectl get pods -n kube-public -o wide   
NAME                       READY     STATUS             RESTARTS   AGE       IP           NODE
nginx-db-c67cdb84c-2btcw   0/1       ImagePullBackOff   0          4m        172.7.22.3   hdss61-22.host.com
nginx-db-c67cdb84c-66jx2   0/1       ImagePullBackOff   0          4m        172.7.21.3   hdss61-21.host.com

[root@hdss61-21 ~]#  kubectl get pods nginx-db-c67cdb84c-2btcw  -o yaml -n kube-public
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: 2020-08-17T15:39:39Z
  generateName: nginx-db-c67cdb84c-

[root@hdss61-22 ~]# kubectl get service -n kube-public  
NAME       CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
nginx-db   192.168.220.66   <none>        80/TCP    1d

[root@hdss61-22 ~]#  kubectl get service nginx-db  -o yaml  -n kube-public   
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2020-08-16T12:42:33Z
  labels:
    app: nginx-db
  name: nginx-db
  namespace: kube-public
  resourceVersion: "232175"
  selfLink: /api/v1/namespaces/kube-public/services/nginx-db
  uid: 2d653347-79a8-4c4d-b03f-52073db27db0
spec:
  clusterIP: 192.168.220.66
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx-db
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}

[root@hdss61-22 ~]#  kubectl explain service.metadata

[root@hdss61-21 ~]# cat  nginx-ds-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ds
  name: nginx-ds
  namespace: default
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx-ds
  sessionAffinity: None
  type: ClusterIP

[root@hdss61-21 ~]#  kubectl create -f nginx-ds-svc.yaml   

[root@hdss61-21 ~]# kubectl get svc -n default
NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes   192.168.0.1      <none>        443/TCP    3d
nginx-ds     192.168.52.225   <none>        8080/TCP   1d

[root@hdss61-21 ~]#  kubectl get svc nginx-ds -o yaml
apiVersion: v1
kind: Service
metadata:
  annotations:

[root@hdss61-21 ~]#  kubectl edit svc nginx-ds  #修改资源配置清单

[root@hdss61-21 ~]#  kubectl get svc  
NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes   192.168.0.1      <none>        443/TCP    3d
nginx-ds     192.168.52.225   <none>        8080/TCP   1d

[root@hdss61-21 ~]# vi nginx-ds-svc.yaml  #离线修改 

[root@hdss61-21 ~]# kubectl apply -f nginx-ds-svc.yaml   

[root@hdss61-21 ~]#  kubectl delete svc nginx-ds  #陈述式删除 

[root@hdss61-21 ~]# kubectl delete -f nginx-ds-svc.yaml  #声明式删除 
service "nginx-ds" deleted    

1.2.7.声明式用法总结
声明式资源管理，依赖于统一资源配置清单文件对资源进行管理
对资源的管理，通过事先定义在统一配置清单内，在通过陈述式-f命令应用到k8s集群里
语法格式：kubectl create/apply/delete -f /path/to/yaml
不懂的，善用explain查询

#########################################################################
[root@hdss-200 ~]# cd  /opt/certs
[root@hdss-200 certs]# cfssl-certinfo  -cert apiserver.pem 
[root@hdss-200 certs]# cfssl-certinfo  -domain  www.baidu.com
[root@hdss-22 conf]# cat  /opt/kubernetes/server/bin/conf/kubelet.kubeconfig 
- name: k8s-node
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lVZWg1VUdFWGlnUjRtcXRibllnZXF5VHBxUTdnd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lERUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEN6QUpCZ05WQkFvVEFtOWtNUXd3Q2dZRFZRUUxFd052Y0hNeEVqQVFCZ05WQkFNVENVOXNaR0p2CmVVVmtkVEFlRncweU1EQTJNVFV4TlRJME1EQmFGdzAwTURBMk1UQXhOVEkwTURCYU1GOHhDekFKQmdOVkJBWVQKQWtOT01SQXdEZ1lEVlFRSUV3ZGlaV2xxYVc1bk1SQXdEZ1lEVlFRSEV3ZGlaV2xxYVc1bk1Rc3dDUVlEVlFRSwpFd0p2WkRFTU1Bb0dBMVVFQ3hNRGIzQnpNUkV3RHdZRFZRUURFd2hyT0hNdGJtOWtaVENDQVNJd0RRWUpLb1pJCmh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFZxTUc0N1JyVlg3NDBncDVNSDJySU9Ob1JvSmNSTTA3Y1AKWndYN2xBbzA1M2E1SnNPUG1kQU1ZVEF5aTRRRWR3M3pFRTQ2SmRDdjFpWHJRYnArdkFCRy9PeHhBMzJHNVV1Kwpqc1Y4WGQ2eTlkM0FkUGQ5ckV1VTlycSt1d01JLzJzcDRJb0RrRzk2V2MzOXZIc3RNSUZBWGJ2Q0FSSjJyd1JhClNMTUpFdEtvOVBvemUvQWJGYkFUWk1aUGhRK3VBVXEzZlI5dEc2cVZidEpLNDc1ZS9vcXVEV1lQYTZhZjJQQUYKanhQYlZXakFNUWJaNjd6RnNCMEdoaDNVdkUzMnBDelNtUGYwaFpLOGVnVVJ0bmFLVUJBUGFjbGp1a25GdmFMSQpGcWxseGFEaTFIUXptRmMvN1huOWxka3hCZkdmMG9LTDBneHJOeU5Ud0VkVG45MStNeXNDQXdFQUFhTjFNSE13CkRnWURWUjBQQVFIL0JBUURBZ1dnTUJNR0ExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUMKTUFBd0hRWURWUjBPQkJZRUZBVVhERk1yNlVPQVlNdi9JbmZGa1hKams0c0JNQjhHQTFVZEl3UVlNQmFBRktyZwpDVnVCNkpTLzIxenFDS1U5NWtSZ0pLWFlNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUFQL1EwRjU4TlJqeFdEClRYVXU2Rmt6SFF2Ym4rT21XZG1Xdy93c2FHMWhSSEpOYmRMc2o1MTNpUDhOMDZuekliMTgvMEpZcEpkeG5TUHMKbll5TmVEV0JucDcxbXhEdkdSREtiZmlLZEpSb2trb1dzd3BtUmt3SGFRTExFdFpkZm0zWTRNWnN4b09rYWVGRgp6TStyaXlMSVBkTWF6TFQxUlNwT1UzQWVyYmJoc2tVQVh1UUp4OGpsa2JLZU1xaFQrUWRGejA1aG9Ha0J6NEFZCkZuVTJHZDVuUmMyMHNRalg3S3ZHRWJlemFMVm1tanE3NFJrSWZoT0NSYXZkRDB5cWxSZitEc3VmUW5Eb21EaVcKbDltUTg5K1B5NXcyN0dYL2Q3ZGZZQWFWREtzRlJUK1hOU2FKMzdBK2EzUFl4Q0xDRHFlRzUzUEEzKzFqWlJnKwpJMkF3Wi9IbgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
[root@hdss-200 certs]#cd /opt/certs
[root@hdss-200 certs]# echo  "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lVZWg1VUdFWGlnUjRtcXRibllnZXF5VHBxUTdnd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lERUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEN6QUpCZ05WQkFvVEFtOWtNUXd3Q2dZRFZRUUxFd052Y0hNeEVqQVFCZ05WQkFNVENVOXNaR0p2CmVVVmtkVEFlRncweU1EQTJNVFV4TlRJME1EQmFGdzAwTURBMk1UQXhOVEkwTURCYU1GOHhDekFKQmdOVkJBWVQKQWtOT01SQXdEZ1lEVlFRSUV3ZGlaV2xxYVc1bk1SQXdEZ1lEVlFRSEV3ZGlaV2xxYVc1bk1Rc3dDUVlEVlFRSwpFd0p2WkRFTU1Bb0dBMVVFQ3hNRGIzQnpNUkV3RHdZRFZRUURFd2hyT0hNdGJtOWtaVENDQVNJd0RRWUpLb1pJCmh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFZxTUc0N1JyVlg3NDBncDVNSDJySU9Ob1JvSmNSTTA3Y1AKWndYN2xBbzA1M2E1SnNPUG1kQU1ZVEF5aTRRRWR3M3pFRTQ2SmRDdjFpWHJRYnArdkFCRy9PeHhBMzJHNVV1Kwpqc1Y4WGQ2eTlkM0FkUGQ5ckV1VTlycSt1d01JLzJzcDRJb0RrRzk2V2MzOXZIc3RNSUZBWGJ2Q0FSSjJyd1JhClNMTUpFdEtvOVBvemUvQWJGYkFUWk1aUGhRK3VBVXEzZlI5dEc2cVZidEpLNDc1ZS9vcXVEV1lQYTZhZjJQQUYKanhQYlZXakFNUWJaNjd6RnNCMEdoaDNVdkUzMnBDelNtUGYwaFpLOGVnVVJ0bmFLVUJBUGFjbGp1a25GdmFMSQpGcWxseGFEaTFIUXptRmMvN1huOWxka3hCZkdmMG9LTDBneHJOeU5Ud0VkVG45MStNeXNDQXdFQUFhTjFNSE13CkRnWURWUjBQQVFIL0JBUURBZ1dnTUJNR0ExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01Bd0dBMVVkRXdFQi93UUMKTUFBd0hRWURWUjBPQkJZRUZBVVhERk1yNlVPQVlNdi9JbmZGa1hKams0c0JNQjhHQTFVZEl3UVlNQmFBRktyZwpDVnVCNkpTLzIxenFDS1U5NWtSZ0pLWFlNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUFQL1EwRjU4TlJqeFdEClRYVXU2Rmt6SFF2Ym4rT21XZG1Xdy93c2FHMWhSSEpOYmRMc2o1MTNpUDhOMDZuekliMTgvMEpZcEpkeG5TUHMKbll5TmVEV0JucDcxbXhEdkdSREtiZmlLZEpSb2trb1dzd3BtUmt3SGFRTExFdFpkZm0zWTRNWnN4b09rYWVGRgp6TStyaXlMSVBkTWF6TFQxUlNwT1UzQWVyYmJoc2tVQVh1UUp4OGpsa2JLZU1xaFQrUWRGejA1aG9Ha0J6NEFZCkZuVTJHZDVuUmMyMHNRalg3S3ZHRWJlemFMVm1tanE3NFJrSWZoT0NSYXZkRDB5cWxSZitEc3VmUW5Eb21EaVcKbDltUTg5K1B5NXcyN0dYL2Q3ZGZZQWFWREtzRlJUK1hOU2FKMzdBK2EzUFl4Q0xDRHFlRzUzUEEzKzFqWlJnKwpJMkF3Wi9IbgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="|base64 -d > 123.pem 
[root@hdss-200 certs]# cfssl-certinfo  -cert 123.pem 
##########################################################################
[root@hdss61-200 certs]# pwd
/opt/certs
[root@hdss61-200 certs]# cfssl-certinfo  -cert  apiserver.pem 
{
  "subject": {
    "common_name": "k8s-apiserver",
    "country": "CN",
    "organization": "od",
    "organizational_unit": "ops",
    "locality": "beijing",
    "province": "beijing",
    "names": [
      "CN",
      "beijing",
      "beijing",
      "od",
      "ops",
      "k8s-apiserver"
    ]
  },
  "issuer": {
    "common_name": "OldboyEdu",
    "country": "CN",
    "organization": "od",
    "organizational_unit": "ops",
    "locality": "BeiJing",
    "province": "BeiJing",
    "names": [
      "CN",
      "BeiJing",
      "BeiJing",
      "od",
      "ops",
      "OldboyEdu"
    ]
  },
  "serial_number": "236988111172538868836791078516843323360445287415",
  "sans": [
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local",
    "127.0.0.1",
    "192.168.0.1",
    "192.168.61.110",   #注意这个vip地址一定要加不然报kubectl  get  nodes no resource 
    "192.168.61.119",
    "192.168.61.120"
  ],
  "not_before": "2020-08-16T07:26:00Z",
  "not_after": "2040-08-11T07:26:00Z",     #k8s证书有效期20年
  "sigalg": "SHA256WithRSA",
  "authority_key_id": "87:40:6B:12:6A:D3:4B:49:CD:E0:F4:E7:E2:F5:FD:1D:A7:34:A:E9",
  "subject_key_id": "17:F9:FC:4A:2E:9C:99:E5:18:5B:14:3F:66:8D:CE:BB:4:78:78:6A",
  "pem": "-----BEGIN CERTIFICATE-----\nMIIEaTCCA1GgAwIBAgIUKYLqGL0IYph5WKfe2FwVLyNxS/cwDQYJKoZIhvcNAQEL\nBQAwYDELMAkGA1UEBhMCQ04xEDAOBgNVBAgTB0JlaUppbmcxEDAOBgNVBAcTB0Jl\naUppbmcxCzAJBgNVBAoTAm9kMQwwCgYDVQQLEwNvcHMxEjAQBgNVBAMTCU9sZGJv\neUVkdTAeFw0yMDA4MTYwNzI2MDBaFw00MDA4MTEwNzI2MDBaMGQxCzAJBgNVBAYT\nAkNOMRAwDgYDVQQIEwdiZWlqaW5nMRAwDgYDVQQHEwdiZWlqaW5nMQswCQYDVQQK\nEwJvZDEMMAoGA1UECxMDb3BzMRYwFAYDVQQDEw1rOHMtYXBpc2VydmVyMIIBIjAN\nBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwRpxAHEcskS6NxrrNuWYb99FlkFG\nUjOB2sVclU1sIgNbzVS76OXmFN1nPzcL870YfPhukb6KtYvwITiRSL+QoKp0B2no\ndgg+5bncC5WC2VAWKUv0Qp9T2350m8BLGOwBULe6CD6Dabv24MbBO1DA1VL5uHG5\nvIIszzAc5Lfo2qCl8DThg30Cr0L2BAgzkUW/yE3mMUF9HIFNPyAC42d4n6nI1G11\nQicaCUusFOI1dulve726dhk0aa+R65G6bU1v66lcZw78KW46IQpHZd5waaEQGB0J\ndmeTOm2PdltuJ4OyGfDEB+zisRtKG0htuVtgn2Uqk363qXN3rG5m4ZAWmwIDAQAB\no4IBFTCCAREwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoGCCsGAQUFBwMBMAwG\nA1UdEwEB/wQCMAAwHQYDVR0OBBYEFBf5/EounJnlGFsUP2aNzrsEeHhqMB8GA1Ud\nIwQYMBaAFIdAaxJq00tJzeD05+L1/R2nNArpMIGbBgNVHREEgZMwgZCCEmt1YmVy\nbmV0ZXMuZGVmYXVsdIIWa3ViZXJuZXRlcy5kZWZhdWx0LnN2Y4Iea3ViZXJuZXRl\ncy5kZWZhdWx0LnN2Yy5jbHVzdGVygiRrdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNs\ndXN0ZXIubG9jYWyHBH8AAAGHBMCoAAGHBMCoPW6HBMCoPXeHBMCoPXgwDQYJKoZI\nhvcNAQELBQADggEBAJvPaGRSegmyn4oOXPFXwyuWcnw5tvUaJkGv0yhLhjpS96wh\nEuxzjwAopxsBXYDlO+zkTUcNaQfILBQiC7yorVMK/7eL5gEAOtEyT6vkpcw8knhc\nvZ8SqbHN4m+BmdnG32WC5JX6nR/XVJOwFQ/qEX5uvPmOCrXjdiz0vrbOJaCiGw5R\nIMwoUGNyER9JrX+5o68qkg0VxKdpA7WdinApeKcwRh8Ub6E9XkOaBy3XOoStv45U\nfXoDpvcgGqHIDqwtacp3QS0AiU8pt/W7NKHabzQANmr3sbnqG5gfRM0JGEPmcb6N\ngl/z8q/bgCFA8qGIRWycxjZkDzmU2ZQF7mMWxvE=\n-----END CERTIFICATE-----\n"
}
################################################################################
1.K8S的CNI网络插件-Flannel

[root@hdss61-21/22 ~]# cd  /opt/src  
wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 
mkdir /opt/flannel-v0.11.0 
tar xf flannel-v0.11.0-linux-amd64.tar.gz  -C /opt/flannel-v0.11.0/  
ln -s /opt/flannel-v0.11.0/ /opt/flannel  
mkdir /opt/flannel/cert  
scp hdss61-200:/opt/certs/ca.pem .  
scp hdss61-200:/opt/certs/client.pem .
scp hdss61-200:/opt/certs/client-key.pem . 
[root@hdss61-21 flannel]# cat  subnet.env 
FLANNEL_NETWORK=172.7.0.0/16
FLANNEL_SUBNET=172.7.21.1/24
FLANNEL_MTU=1500
FLANNEL_IPMASQ=false
[root@hdss61-21 flannel]# cat  flanneld.sh 
#!/bin/sh
./flanneld \
  --public-ip=192.168.61.119 \
  --etcd-endpoints=https://192.168.61.117:2379,https://192.168.61.119:2379,https://192.168.61.120:2379 \
  --etcd-keyfile=./cert/client-key.pem \
  --etcd-certfile=./cert/client.pem \
  --etcd-cafile=./cert/ca.pem \
  --iface=eth0 \
  --subnet-file=./subnet.env \
  --healthz-port=2401
chmod +x flanneld.sh 
mkdir -p /data/logs/flanneld  
[root@hdss61-21 flannel]# cat   /etc/supervisord.d/flannel.ini 
[program:flanneld-61-21]
command=/opt/flannel/flanneld.sh                             ; the program (relative uses PATH, can take args)
numprocs=1                                                   ; number of processes copies to start (def 1)
directory=/opt/flannel                                       ; directory to cwd to before exec (def no cwd)
autostart=true                                               ; start at supervisord start (default: true)
autorestart=true                                             ; retstart at unexpected quit (default: true)
startsecs=30                                                 ; number of secs prog must stay running (def. 1)
startretries=3                                               ; max # of serial start failures (default 3)
exitcodes=0,2                                                ; 'expected' exit codes for process (default 0,2)
stopsignal=QUIT                                              ; signal used to kill process (default TERM)
stopwaitsecs=10                                              ; max num secs to wait b4 SIGKILL (default 10)
user=root                                                    ; setuid to this UNIX account to run the program
redirect_stderr=true                                         ; redirect proc stderr to stdout (default false)
stdout_logfile=/data/logs/flanneld/flanneld.stdout.log       ; stderr log path, NONE for none; default AUTO
stdout_logfile_maxbytes=64MB                                 ; max # logfile bytes b4 rotation (default 50MB)
stdout_logfile_backups=4                                     ; # of stdout logfile backups (default 10)
stdout_capture_maxbytes=1MB                                  ; number of bytes in 'capturemode' (default 0)
stdout_events_enabled=false                                  ; emit events on stdout writes (default false)
[root@hdss61-21 etcd]# ./etcdctl set /coreos.com/network/config '{"Network": "172.7.0.0/16", "Backend": {"Type": "host-gw"}}'
[root@hdss61-21 etcd]#  ./etcdctl get /coreos.com/network/config 
{"Network": "172.7.0.0/16", "Backend": {"Type": "host-gw"}}
[root@hdss61-21 etcd]# supervisorctl  update 
[root@hdss61-21 etcd]# supervisorctl  status 
etcd-server-61-21                RUNNING   pid 28755, uptime 1 day, 8:28:12
flanneld-61-21                   RUNNING   pid 29222, uptime 22:23:35
kube-apiserver-61-21             RUNNING   pid 28754, uptime 1 day, 8:28:12
kube-controller-manager-61-21    RUNNING   pid 28761, uptime 1 day, 8:28:12
kube-kubelet-61-21               RUNNING   pid 28756, uptime 1 day, 8:28:12
kube-proxy-61-21                 RUNNING   pid 28759, uptime 1 day, 8:28:12
kube-scheduler-61-21             RUNNING   pid 28757, uptime 1 day, 8:28:12
[root@hdss61-21 flannel]# tail -fn 200 /data/logs/flanneld/flanneld.stdout.log 
其他节点基本和hdss7-21相同，注意修改一下文件：
subnet.env
flanneld.sh
/etc/supervisord.d/flannel.ini
[root@hdss61-21 flannel]# kubectl  get  pods  -o  wide 
NAME             READY     STATUS    RESTARTS   AGE       IP           NODE
nginx-ds-9kbmj   1/1       Running   0          1d        172.7.21.2   hdss61-21.host.com
nginx-ds-ks6cp   1/1       Running   0          1d        172.7.22.2   hdss61-22.host.com
[root@hdss61-21 flannel]#  ping 172.7.22.2
PING 172.7.22.2 (172.7.22.2) 56(84) bytes of data.
64 bytes from 172.7.22.2: icmp_seq=1 ttl=63 time=0.767 ms
64 bytes from 172.7.22.2: icmp_seq=2 ttl=63 time=0.565 ms
^C
--- 172.7.22.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.565/0.666/0.767/0.101 ms
[root@hdss61-21 flannel]#  ping 172.7.21.2
PING 172.7.21.2 (172.7.21.2) 56(84) bytes of data.
64 bytes from 172.7.21.2: icmp_seq=1 ttl=64 time=0.121 ms
64 bytes from 172.7.21.2: icmp_seq=2 ttl=64 time=0.074 ms













  



